{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration Loss for ANCOVA NPE Training\n",
    "\n",
    "This notebook compares two training approaches for the ANCOVA 2-arm continuous outcome model:\n",
    "\n",
    "1. **Baseline** — Standard BayesFlow training with negative log-likelihood (NLL) loss only\n",
    "2. **Calibrated** — NLL + differentiable calibration loss from [Falkner et al. (NeurIPS 2023)](https://arxiv.org/abs/2310.13402)\n",
    "\n",
    "Both models use the **same architecture and training settings**. The only difference is the\n",
    "calibration loss term, which penalizes under-coverage during training.\n",
    "\n",
    "The calibration loss package lives at https://github.com/matthiaskloft/bfcalloss and is installed via:\n",
    "```bash\n",
    "pip install -e \".[calibration]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.environ.get(\"KERAS_BACKEND\"):\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "\n",
    "# ANCOVA model\n",
    "from rctbp_bf_training.models.ancova.model import (\n",
    "    ANCOVAConfig,\n",
    "    create_ancova_adapter,\n",
    "    create_simulator,\n",
    "    create_ancova_workflow_components,\n",
    "    create_validation_grid,\n",
    "    make_simulate_fn,\n",
    ")\n",
    "from rctbp_bf_training.core.infrastructure import (\n",
    "    build_summary_network,\n",
    "    build_inference_network,\n",
    ")\n",
    "from rctbp_bf_training.core.utils import MovingAverageEarlyStopping\n",
    "from rctbp_bf_training.core.validation import (\n",
    "    run_validation_pipeline,\n",
    "    make_bayesflow_infer_fn,\n",
    ")\n",
    "\n",
    "# Calibration loss (from bfcalloss)\n",
    "from bayesflow_calibration import (\n",
    "    CalibratedContinuousApproximator,\n",
    "    CalibrationMonitorCallback,\n",
    "    GammaSchedule,\n",
    ")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "RNG = np.random.default_rng(2025)\n",
    "\n",
    "print(f\"BayesFlow {bf.__version__}\")\n",
    "print(f\"Keras {keras.__version__} (backend: {keras.backend.backend()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Shared Configuration\n",
    "\n",
    "Both models share the same ANCOVA config, simulator, adapter, and network architecture.\n",
    "We use a smaller architecture for faster iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ANCOVAConfig()\n",
    "\n",
    "# Use the config defaults for network architecture\n",
    "print(\"Summary network:\", config.workflow.summary_network)\n",
    "print(\"Inference network:\", config.workflow.inference_network)\n",
    "print(\"Training:\", config.workflow.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulator and adapter (shared between both models)\n",
    "simulator = create_simulator(config, RNG)\n",
    "adapter = create_ancova_adapter()\n",
    "\n",
    "# Quick test\n",
    "sim_test = simulator.sample(10)\n",
    "processed = adapter(sim_test)\n",
    "print(\"summary_variables:\", processed[\"summary_variables\"].shape)\n",
    "print(\"inference_variables:\", processed[\"inference_variables\"].shape)\n",
    "print(\"inference_conditions:\", processed[\"inference_conditions\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shared Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters (same for both models)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "BATCHES_PER_EPOCH = 50\n",
    "VALIDATION_SIMS = 500\n",
    "\n",
    "train_config = config.workflow.training\n",
    "\n",
    "\n",
    "def make_optimizer():\n",
    "    \"\"\"Create a fresh optimizer (must be separate per model).\"\"\"\n",
    "    steps_per_epoch = BATCH_SIZE * BATCHES_PER_EPOCH\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=train_config.initial_lr,\n",
    "        decay_steps=steps_per_epoch,\n",
    "        decay_rate=train_config.decay_rate,\n",
    "        staircase=True,\n",
    "    )\n",
    "    return keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "def make_early_stopping():\n",
    "    \"\"\"Create a fresh early stopping callback.\"\"\"\n",
    "    return MovingAverageEarlyStopping(\n",
    "        window=train_config.early_stopping_window,\n",
    "        patience=train_config.early_stopping_patience,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Model (NLL only)\n",
    "\n",
    "Standard BayesFlow training — the normalizing flow is trained to maximize the log-density\n",
    "of the true parameters under the learned posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fresh networks for the baseline\n",
    "summary_net_base, inference_net_base, _ = create_ancova_workflow_components(config)\n",
    "\n",
    "workflow_base = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_net_base,\n",
    "    summary_network=summary_net_base,\n",
    "    optimizer=make_optimizer(),\n",
    "    inference_conditions=[\"N\", \"p_alloc\", \"prior_df\", \"prior_scale\"],\n",
    ")\n",
    "workflow_base.approximator.compile(optimizer=make_optimizer())\n",
    "\n",
    "print(\"Baseline workflow created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_base = workflow_base.fit_online(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH,\n",
    "    validation_data=VALIDATION_SIMS,\n",
    "    callbacks=[make_early_stopping()],\n",
    ")\n",
    "print(\"Baseline training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Calibrated Model (NLL + Calibration Loss)\n",
    "\n",
    "Uses `CalibratedContinuousApproximator` from the `bayesflow-calibration` package.\n",
    "This subclasses BayesFlow's `ContinuousApproximator` and injects a calibration loss\n",
    "term that penalizes under-coverage during training.\n",
    "\n",
    "Key settings:\n",
    "- **`gamma_schedule`**: linear warmup — start with pure NLL, ramp up calibration pressure over 20 epochs\n",
    "- **`calibration_mode=0.0`**: conservativeness mode (penalize under-coverage only)\n",
    "- **`n_rank_samples=100`**: prior samples for rank computation\n",
    "- **`subsample_size=80`**: subsample batch for calibration loss (reduces overhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fresh networks for the calibrated model\n",
    "summary_net_cal, inference_net_cal, _ = create_ancova_workflow_components(config)\n",
    "\n",
    "\n",
    "def ancova_prior_fn(n_samples):\n",
    "    \"\"\"Sample b_group from the ANCOVA marginal prior.\n",
    "\n",
    "    Integrates over meta-parameters (prior_df, prior_scale) to get\n",
    "    the marginal prior distribution of b_group.\n",
    "    Must return np.ndarray of shape (n_samples, param_dim).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "    samples = np.zeros((n_samples, 1))\n",
    "    for j in range(n_samples):\n",
    "        # Sample meta-parameters from their priors\n",
    "        prior_df = int(round(\n",
    "            rng.integers(0, config.meta.prior_df_max + 1)\n",
    "        ))\n",
    "        prior_scale = rng.gamma(\n",
    "            shape=config.meta.prior_scale_gamma_shape,\n",
    "            scale=config.meta.prior_scale_gamma_scale,\n",
    "        )\n",
    "        # Sample b_group from the conditional prior\n",
    "        from rctbp_bf_training.core.utils import sample_t_or_normal\n",
    "        samples[j, 0] = sample_t_or_normal(\n",
    "            df=prior_df, scale=prior_scale, rng=rng\n",
    "        )\n",
    "    return samples.astype(np.float32)\n",
    "\n",
    "\n",
    "# Quick check: sample from the marginal prior\n",
    "test_prior = ancova_prior_fn(1000)\n",
    "print(f\"Marginal prior: mean={test_prior.mean():.3f}, std={test_prior.std():.3f}, shape={test_prior.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the calibrated approximator\n",
    "gamma_schedule = GammaSchedule(\n",
    "    schedule_type=\"linear_warmup\",\n",
    "    gamma_max=100.0,\n",
    "    warmup_epochs=20,\n",
    "    gamma_min=0.0,\n",
    ")\n",
    "\n",
    "approximator_cal = CalibratedContinuousApproximator(\n",
    "    # BayesFlow ContinuousApproximator args\n",
    "    inference_network=inference_net_cal,\n",
    "    summary_network=summary_net_cal,\n",
    "    # Calibration-specific args\n",
    "    prior_fn=ancova_prior_fn,\n",
    "    gamma_schedule=gamma_schedule,\n",
    "    calibration_mode=0.0,       # conservativeness: penalize under-coverage only\n",
    "    n_rank_samples=100,         # prior samples for rank computation\n",
    "    subsample_size=80,          # subsample batch for efficiency\n",
    ")\n",
    "\n",
    "approximator_cal.compile(optimizer=make_optimizer())\n",
    "print(\"Calibrated approximator created\")\n",
    "print(f\"  gamma schedule: {gamma_schedule.schedule_type}, max={gamma_schedule.gamma_max}, warmup={gamma_schedule.warmup_epochs}\")\n",
    "print(f\"  calibration_mode: {approximator_cal.calibration_mode}\")\n",
    "print(f\"  n_rank_samples: {approximator_cal.n_rank_samples}\")\n",
    "print(f\"  subsample_size: {approximator_cal.subsample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use BasicWorkflow for the calibrated model since it uses\n",
    "# ContinuousApproximator internally. Instead, we train the approximator\n",
    "# directly using BayesFlow's online training loop.\n",
    "\n",
    "# Create a BasicWorkflow just for the data pipeline, but swap the approximator\n",
    "workflow_cal = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=inference_net_cal,\n",
    "    summary_network=summary_net_cal,\n",
    "    optimizer=make_optimizer(),\n",
    "    inference_conditions=[\"N\", \"p_alloc\", \"prior_df\", \"prior_scale\"],\n",
    ")\n",
    "# Replace the approximator with our calibrated one\n",
    "workflow_cal.approximator = approximator_cal\n",
    "\n",
    "# Train with CalibrationMonitorCallback (REQUIRED for gamma scheduling)\n",
    "history_cal = workflow_cal.fit_online(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH,\n",
    "    validation_data=VALIDATION_SIMS,\n",
    "    callbacks=[make_early_stopping(), CalibrationMonitorCallback()],\n",
    ")\n",
    "print(\"Calibrated training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loss Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss\n",
    "ax = axes[0]\n",
    "ax.plot(history_base.history[\"loss\"], label=\"Baseline (NLL)\", alpha=0.8)\n",
    "ax.plot(history_cal.history[\"loss\"], label=\"Calibrated (NLL + cal)\", alpha=0.8)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Training Loss\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "ax = axes[1]\n",
    "ax.plot(history_base.history.get(\"val_loss\", []), label=\"Baseline\", alpha=0.8)\n",
    "ax.plot(history_cal.history.get(\"val_loss\", []), label=\"Calibrated\", alpha=0.8)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Validation Loss\")\n",
    "ax.set_title(\"Validation Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BayesFlow Built-in Diagnostics\n",
    "\n",
    "Quick comparison using BayesFlow's default diagnostic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shared validation data\n",
    "val_sims = simulator.sample(1000)\n",
    "\n",
    "# Compute default diagnostics for both models\n",
    "metrics_base = workflow_base.compute_default_diagnostics(test_data=val_sims)\n",
    "metrics_cal = workflow_cal.compute_default_diagnostics(test_data=val_sims)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Baseline\": metrics_base[\"b_group\"],\n",
    "    \"Calibrated\": metrics_cal[\"b_group\"],\n",
    "})\n",
    "print(\"BayesFlow default diagnostics (b_group):\")\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation on Conditions Grid\n",
    "\n",
    "Systematic comparison across a grid of ANCOVA conditions (N, prior_df, prior_scale, b_group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the reduced validation grid (faster)\n",
    "conditions = create_validation_grid(extended=False)\n",
    "print(f\"Validation grid: {len(conditions)} conditions\")\n",
    "print(f\"Example: {conditions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation for baseline model\n",
    "simulate_fn = make_simulate_fn(rng=RNG)\n",
    "\n",
    "infer_fn_base = make_bayesflow_infer_fn(\n",
    "    workflow_base.approximator,\n",
    "    param_key=\"b_group\",\n",
    "    data_keys=[\"outcome\", \"covariate\", \"group\"],\n",
    "    context_keys={\"N\": int, \"p_alloc\": float, \"prior_df\": float, \"prior_scale\": float},\n",
    ")\n",
    "\n",
    "print(\"=== Baseline Model ===\")\n",
    "results_base = run_validation_pipeline(\n",
    "    conditions_list=conditions,\n",
    "    n_sims=500,\n",
    "    n_post_draws=500,\n",
    "    simulate_fn=simulate_fn,\n",
    "    infer_fn=infer_fn_base,\n",
    "    true_param_key=\"b_arm_treat\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation for calibrated model\n",
    "infer_fn_cal = make_bayesflow_infer_fn(\n",
    "    workflow_cal.approximator,\n",
    "    param_key=\"b_group\",\n",
    "    data_keys=[\"outcome\", \"covariate\", \"group\"],\n",
    "    context_keys={\"N\": int, \"p_alloc\": float, \"prior_df\": float, \"prior_scale\": float},\n",
    ")\n",
    "\n",
    "print(\"=== Calibrated Model ===\")\n",
    "results_cal = run_validation_pipeline(\n",
    "    conditions_list=conditions,\n",
    "    n_sims=500,\n",
    "    n_post_draws=500,\n",
    "    simulate_fn=simulate_fn,\n",
    "    infer_fn=infer_fn_cal,\n",
    "    true_param_key=\"b_arm_treat\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_base = results_base[\"metrics\"][\"summary\"]\n",
    "s_cal = results_cal[\"metrics\"][\"summary\"]\n",
    "\n",
    "summary_keys = [\n",
    "    \"recovery_corr\", \"recovery_r2\", \"overall_nrmse\", \"overall_bias\",\n",
    "    \"mean_contraction\", \"mean_cal_error\",\n",
    "    \"coverage_50\", \"coverage_80\", \"coverage_90\", \"coverage_95\",\n",
    "    \"sbc_ks_pvalue\", \"sbc_c2st_accuracy\",\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": summary_keys,\n",
    "    \"Baseline\": [s_base.get(k, float(\"nan\")) for k in summary_keys],\n",
    "    \"Calibrated\": [s_cal.get(k, float(\"nan\")) for k in summary_keys],\n",
    "}).set_index(\"Metric\")\n",
    "\n",
    "# Format nicely\n",
    "comparison_df[\"Difference\"] = comparison_df[\"Calibrated\"] - comparison_df[\"Baseline\"]\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"         Baseline  vs  Calibrated  (NLL + calibration loss)\")\n",
    "print(\"=\" * 65)\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "print(\"\\nKey:\")\n",
    "print(\"  mean_cal_error: lower is better (0 = perfect calibration)\")\n",
    "print(\"  coverage_XX: closer to XX/100 is better\")\n",
    "print(\"  sbc_ks_pvalue: > 0.05 suggests calibration\")\n",
    "print(\"  sbc_c2st_accuracy: closer to 0.5 is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Coverage Profile Comparison\n",
    "\n",
    "The coverage profile shows empirical vs. nominal coverage at every level from 1% to 99%.\n",
    "A well-calibrated model follows the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (label, results) in zip(axes, [(\"Baseline\", results_base), (\"Calibrated\", results_cal)]):\n",
    "    profile = results[\"metrics\"][\"summary\"][\"coverage_profile\"]\n",
    "    nominal = sorted(profile.keys())\n",
    "    empirical = [profile[n] for n in nominal]\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Perfect calibration\")\n",
    "    ax.plot(nominal, empirical, \"b-\", lw=2, label=\"Empirical\")\n",
    "    ax.fill_between(nominal, empirical, nominal, alpha=0.2, color=\"red\")\n",
    "    ax.set_xlabel(\"Nominal Coverage\")\n",
    "    ax.set_ylabel(\"Empirical Coverage\")\n",
    "    ax.set_title(f\"{label} — Coverage Profile\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    cal_err = results[\"metrics\"][\"summary\"][\"mean_cal_error\"]\n",
    "    ax.text(0.95, 0.05, f\"Cal. Error: {cal_err:.4f}\",\n",
    "            transform=ax.transAxes, ha=\"right\", va=\"bottom\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Per-Condition Calibration Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_base = results_base[\"metrics\"][\"condition_metrics\"]\n",
    "cond_cal = results_cal[\"metrics\"][\"condition_metrics\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Mean calibration error per condition\n",
    "ax = axes[0]\n",
    "x = np.arange(len(cond_base))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, cond_base[\"mean_cal_error\"], w, label=\"Baseline\", alpha=0.8)\n",
    "ax.bar(x + w/2, cond_cal[\"mean_cal_error\"], w, label=\"Calibrated\", alpha=0.8)\n",
    "ax.set_xlabel(\"Condition\")\n",
    "ax.set_ylabel(\"Mean Calibration Error\")\n",
    "ax.set_title(\"Calibration Error by Condition\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# 95% coverage per condition\n",
    "ax = axes[1]\n",
    "ax.bar(x - w/2, cond_base[\"coverage_95\"], w, label=\"Baseline\", alpha=0.8)\n",
    "ax.bar(x + w/2, cond_cal[\"coverage_95\"], w, label=\"Calibrated\", alpha=0.8)\n",
    "ax.axhline(0.95, color=\"red\", ls=\"--\", alpha=0.7, label=\"Nominal (95%)\")\n",
    "ax.set_xlabel(\"Condition\")\n",
    "ax.set_ylabel(\"95% Coverage\")\n",
    "ax.set_title(\"95% Coverage by Condition\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# NRMSE per condition (should not degrade much)\n",
    "ax = axes[2]\n",
    "ax.bar(x - w/2, cond_base[\"nrmse\"], w, label=\"Baseline\", alpha=0.8)\n",
    "ax.bar(x + w/2, cond_cal[\"nrmse\"], w, label=\"Calibrated\", alpha=0.8)\n",
    "ax.set_xlabel(\"Condition\")\n",
    "ax.set_ylabel(\"NRMSE\")\n",
    "ax.set_title(\"NRMSE by Condition (lower is better)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Condition-Level Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline — condition summary:\")\n",
    "display(results_base[\"metrics\"][\"condition_summary\"].round(4))\n",
    "\n",
    "print(\"\\nCalibrated — condition summary:\")\n",
    "display(results_cal[\"metrics\"][\"condition_summary\"].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "**Expected observations:**\n",
    "\n",
    "- The calibrated model should have **lower calibration error** and empirical coverage\n",
    "  closer to nominal levels (especially at 90% and 95%).\n",
    "- In conservativeness mode (`calibration_mode=0.0`), the calibrated model may produce\n",
    "  slightly **wider** credible intervals (higher posterior SD) — this is by design.\n",
    "- The NRMSE and recovery correlation should remain similar, showing the calibration loss\n",
    "  does not significantly harm point estimation accuracy.\n",
    "- The SBC KS p-value should be higher for the calibrated model (closer to uniform rank\n",
    "  distribution), and the C2ST accuracy closer to 0.5.\n",
    "\n",
    "**Notes:**\n",
    "- The calibration loss adds training overhead (roughly 2-6x depending on `n_rank_samples`\n",
    "  and `subsample_size`). Adjust these for your compute budget.\n",
    "- `gamma_schedule` with linear warmup is recommended — starting with pure NLL lets the\n",
    "  network learn a reasonable posterior before calibration pressure is applied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
