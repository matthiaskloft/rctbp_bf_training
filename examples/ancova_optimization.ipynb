{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for 2-Arm ANCOVA NPE Model\n",
    "\n",
    "This notebook performs **Bayesian Optimization** (Optuna) to find optimal neural network architecture for the 2-arm ANCOVA amortized posterior estimation model.\n",
    "\n",
    "## Objectives\n",
    "1. **Minimize calibration error** (mean absolute coverage error)\n",
    "2. **Minimize parameter count** (model size)\n",
    "\n",
    "This is a multi-objective optimization with Pareto-optimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.environ.get(\"KERAS_BACKEND\"):\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "    \n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "RNG = np.random.default_rng(2025)\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ANCOVA-specific functions\n",
    "from rctbp_bf_training.models.ancova.model import (\n",
    "    ANCOVAConfig,\n",
    "    create_ancova_adapter,\n",
    "    create_simulator,\n",
    "    create_validation_grid,\n",
    "    get_model_metadata,\n",
    "    save_model_with_metadata,\n",
    "    create_ancova_objective,\n",
    "    create_ancova_training_functions,\n",
    ")\n",
    "\n",
    "# Import infrastructure (for final model saving)\n",
    "from rctbp_bf_training.core.infrastructure import params_dict_to_workflow_config\n",
    "\n",
    "# Import Bayesian optimization infrastructure\n",
    "from rctbp_bf_training.core import optimization\n",
    "from rctbp_bf_training.core.optimization import (\n",
    "    create_study,\n",
    "    HyperparameterSpace,\n",
    "    get_param_count,\n",
    "    plot_optimization_results,\n",
    "    summarize_best_trials,\n",
    "    train_until_threshold,\n",
    "    QualityThresholds,\n",
    ")\n",
    "\n",
    "# Create default configuration\n",
    "config = ANCOVAConfig()\n",
    "print(f\"Config loaded: ancova_cont_2arms\")\n",
    "print(f\"\\nNetwork configs:\")\n",
    "print(f\"  Summary network: {config.workflow.summary_network}\")\n",
    "print(f\"  Inference network: {config.workflow.inference_network}\")\n",
    "print(f\"\\nOptuna available: {optimization.OPTUNA_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulator and adapter (prerequisites for optimization)\n",
    "simulator = create_simulator(config, RNG)\n",
    "adapter = create_ancova_adapter()\n",
    "\n",
    "print(\"Setup complete:\")\n",
    "print(f\"  Simulator ready with {len(simulator.sample(1).keys())} output keys\")\n",
    "print(f\"  Adapter configured for ANCOVA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Space and Optimization Grid\n",
    "\n",
    "Define the hyperparameter search space and a reduced validation grid for faster optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space (customize ranges as needed)\n",
    "search_space = HyperparameterSpace(\n",
    "    # DeepSet\n",
    "    summary_dim=(4, 16),\n",
    "    deepset_width=(32, 128),\n",
    "    deepset_depth=(1, 4),\n",
    "    deepset_dropout=(0.05, 0.5),\n",
    "    \n",
    "    # CouplingFlow  \n",
    "    flow_depth=(2, 8),\n",
    "    flow_hidden=(32, 128),\n",
    "    flow_dropout=(0.05, 0.5),\n",
    "    \n",
    "    # Training\n",
    "    initial_lr=(1e-5, 5e-3),\n",
    "    batch_size=(128, 832),\n",
    "    \n",
    "    # Fixed (not optimized)\n",
    "    decay_rate=0.85,\n",
    "    patience=10,\n",
    "    window=20,\n",
    ")\n",
    "\n",
    "# Use factory function for validation grid\n",
    "opt_conditions = create_validation_grid(extended=True)\n",
    "\n",
    "print(f\"Search space defined with {len(search_space.__dataclass_fields__)} parameters\")\n",
    "print(f\"Optimization validation grid: {len(opt_conditions)} conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create objective function using factory\n",
    "objective = create_ancova_objective(\n",
    "    config=config,\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    search_space=search_space,\n",
    "    validation_conditions=opt_conditions,\n",
    "    n_sims=500,\n",
    "    n_post_draws=500,\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "print(\"Objective function created via factory\")\n",
    "print(f\"  Validation grid: {len(opt_conditions)} conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Study\n",
    "\n",
    "Create the multi-objective Optuna study. Results are saved to SQLite for resumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-11 15:58:16,780] A new study created in RDB with name: ancova_cont_2arms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study created: ancova_cont_2arms\n",
      "Existing trials: 0\n",
      "Directions: [<StudyDirection.MINIMIZE: 1>, <StudyDirection.MINIMIZE: 1>]\n"
     ]
    }
   ],
   "source": [
    "# Create multi-objective study\n",
    "study = create_study(\n",
    "    study_name=\"ancova_cont_2arms\",\n",
    "    directions=[\"minimize\", \"minimize\"],  # [calibration_error, param_count]\n",
    "    storage=\"sqlite:///optuna_ancova_cont_2arms.db\",  # Persistent storage for resumption\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "print(f\"Study created: {study.study_name}\")\n",
    "print(f\"Existing trials: {len(study.trials)}\")\n",
    "print(f\"Directions: {study.directions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard command:\n",
      "  python -m rctbp_bf_training.core.optimization --dashboard sqlite:///optuna_ancova_cont_2arms.db\n"
     ]
    }
   ],
   "source": [
    "# To monitor optimization in real-time, launch the Optuna Dashboard from a separate terminal:\n",
    "#\n",
    "#   python -m rctbp_bf_training.core.optimization --dashboard sqlite:///optuna_ancova_cont_2arms.db\n",
    "#\n",
    "# Or from Python:\n",
    "#   from rctbp_bf_training.core.optimization import launch_dashboard\n",
    "#   launch_dashboard(\"sqlite:///optuna_ancova_cont_2arms.db\")\n",
    "#\n",
    "# Requires: pip install optuna-dashboard\n",
    "\n",
    "print(\"Dashboard command:\")\n",
    "print(\"  python -m rctbp_bf_training.core.optimization --dashboard sqlite:///optuna_ancova_cont_2arms.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optimization\n",
    "\n",
    "Run the multi-objective optimization study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cce894537e744ea944e9dfb142ecdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 7s/step - loss: 3.2609 - val_loss: 1.0902 - moving_avg_val_loss: 1.0902\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 6s/step - loss: 3.5742 - val_loss: 1.2869 - moving_avg_val_loss: 1.1885\n",
      "Epoch 3/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 6s/step - loss: 9.4529 - val_loss: 2.4667 - moving_avg_val_loss: 1.6146\n",
      "Epoch 4/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 6s/step - loss: 1.2659 - val_loss: 0.3708 - moving_avg_val_loss: 1.3036\n",
      "Epoch 5/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 5s/step - loss: 6.8447 - val_loss: -0.3281 - moving_avg_val_loss: 0.9773\n",
      "Epoch 6/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 4s/step - loss: 0.6076 - val_loss: 0.9928 - moving_avg_val_loss: 0.9799\n",
      "Epoch 7/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 6s/step - loss: 2.2132 - val_loss: -0.8191 - moving_avg_val_loss: 0.7229\n",
      "Epoch 8/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 7s/step - loss: -0.4092 - val_loss: -0.7704 - moving_avg_val_loss: 0.5362\n",
      "Epoch 9/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 6s/step - loss: 2261.0906 - val_loss: -0.2707 - moving_avg_val_loss: 0.4466\n",
      "Epoch 10/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 5s/step - loss: -0.2633 - val_loss: -1.3165 - moving_avg_val_loss: 0.2702\n",
      "Epoch 11/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 7s/step - loss: -1.0152 - val_loss: -1.6375 - moving_avg_val_loss: 0.0968\n",
      "Epoch 12/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 6s/step - loss: -1.2583 - val_loss: -2.0953 - moving_avg_val_loss: -0.0859\n",
      "Epoch 13/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 6s/step - loss: -2.0451 - val_loss: -2.3247 - moving_avg_val_loss: -0.2581\n",
      "Epoch 14/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 7s/step - loss: -2.3578 - val_loss: -2.8455 - moving_avg_val_loss: -0.4429\n",
      "Epoch 15/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 7s/step - loss: -2.3041 - val_loss: -2.8940 - moving_avg_val_loss: -0.6063\n",
      "Epoch 16/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 7s/step - loss: -2.1940 - val_loss: -2.6649 - moving_avg_val_loss: -0.7350\n",
      "Epoch 17/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 8s/step - loss: -1.6725 - val_loss: -0.9945 - moving_avg_val_loss: -0.7502\n",
      "Epoch 18/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 8s/step - loss: -1.7626 - val_loss: -2.4884 - moving_avg_val_loss: -0.8468\n",
      "Epoch 19/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 6s/step - loss: -2.5230 - val_loss: -2.9631 - moving_avg_val_loss: -0.9582\n",
      "Epoch 20/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 7s/step - loss: -2.7739 - val_loss: -2.9466 - moving_avg_val_loss: -1.0576\n",
      "Epoch 21/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 8s/step - loss: -3.0611 - val_loss: -3.6228 - moving_avg_val_loss: -1.2933\n",
      "Epoch 22/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 7s/step - loss: -2.4694 - val_loss: -2.4031 - moving_avg_val_loss: -1.4778\n",
      "Epoch 23/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 7s/step - loss: -2.7543 - val_loss: -3.4290 - moving_avg_val_loss: -1.7725\n",
      "Epoch 24/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 8s/step - loss: -3.3466 - val_loss: -3.8522 - moving_avg_val_loss: -1.9837\n",
      "Epoch 25/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 6s/step - loss: -2.0538 - val_loss: -3.2276 - moving_avg_val_loss: -2.1287\n",
      "Epoch 26/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 7s/step - loss: -3.3059 - val_loss: -3.8526 - moving_avg_val_loss: -2.3709\n",
      "Epoch 27/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 6s/step - loss: -3.1832 - val_loss: -3.4746 - moving_avg_val_loss: -2.5037\n",
      "Epoch 28/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 6s/step - loss: -3.3766 - val_loss: -3.8736 - moving_avg_val_loss: -2.6589\n",
      "Epoch 29/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 8s/step - loss: -3.6704 - val_loss: -2.4382 - moving_avg_val_loss: -2.7672\n",
      "Epoch 30/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 5s/step - loss: -2.1414 - val_loss: -3.3200 - moving_avg_val_loss: -2.8674\n",
      "Epoch 31/200\n",
      "\u001b[1m 2/50\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 2s/step - loss: -3.0884[W 2026-01-11 18:42:15,204] Trial 0 failed with parameters: {'summary_dim': 8, 'deepset_width': 128, 'deepset_depth': 3, 'deepset_dropout': 0.31939631788866646, 'flow_depth': 3, 'flow_hidden': 48, 'flow_dropout': 0.07613762547568977, 'initial_lr': 0.002176624112345368, 'batch_size': 576} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\src\\rctbp_bf_training\\core\\optimization.py\", line 1118, in objective\n",
      "    history = wf.fit_online(\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py\", line 789, in fit_online\n",
      "    return self._fit(\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py\", line 966, in _fit\n",
      "    self.history = self.approximator.fit(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py\", line 315, in fit\n",
      "    return super().fit(*args, **kwargs, adapter=self.adapter)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\approximator.py\", line 139, in fit\n",
      "    return super().fit(dataset=dataset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\backend_approximator.py\", line 20, in fit\n",
      "    return super().fit(x=dataset, y=None, **filter_kwargs(kwargs, super().fit))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py\", line 269, in fit\n",
      "    logs = self.train_function(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py\", line 124, in one_step_on_data\n",
      "    return self.train_step(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\torch_approximator.py\", line 101, in train_step\n",
      "    self.optimizer.apply(gradients, trainable_weights)\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 526, in apply\n",
      "    self._backend_apply_gradients(grads, trainable_variables)\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 593, in _backend_apply_gradients\n",
      "    grads, trainable_variables, self.learning_rate\n",
      "                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 712, in learning_rate\n",
      "    return self._get_current_learning_rate()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 789, in _get_current_learning_rate\n",
      "    return self._learning_rate(self._iterations)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\schedules\\learning_rate_schedule.py\", line 164, in __call__\n",
      "    initial_learning_rate = ops.convert_to_tensor(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\ops\\core.py\", line 999, in convert_to_tensor\n",
      "    return backend.core.convert_to_tensor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\core.py\", line 212, in convert_to_tensor\n",
      "    return torch.as_tensor(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2026-01-11 18:42:15,393] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run optimization (adjust n_trials based on compute budget)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Each trial takes ~5-10 minutes depending on architecture\u001b[39;00m\n\u001b[32m      3\u001b[39m N_TRIALS = \u001b[32m100\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptimization complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal trials: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study.trials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\rctbp_bf_training\\src\\rctbp_bf_training\\core\\optimization.py:1118\u001b[39m, in \u001b[36mcreate_optimization_objective.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     history = \u001b[43mwf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_batches_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatches_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_sims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1126\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m FAILED: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py:789\u001b[39m, in \u001b[36mBasicWorkflow.fit_online\u001b[39m\u001b[34m(self, epochs, num_batches_per_epoch, batch_size, keep_optimizer, validation_data, augmentations, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    742\u001b[39m \u001b[33;03mTrain the approximator using an online data-generating process. The dataset is dynamically generated during\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[33;03mtraining, making this approach suitable for scenarios where generating new simulations is computationally cheap.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    778\u001b[39m \u001b[33;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m dataset = OnlineDataset(\n\u001b[32m    782\u001b[39m     simulator=\u001b[38;5;28mself\u001b[39m.simulator,\n\u001b[32m    783\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     augmentations=augmentations,\n\u001b[32m    787\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py:966\u001b[39m, in \u001b[36mBasicWorkflow._fit\u001b[39m\u001b[34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[39m\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mself\u001b[39m.approximator.compile(optimizer=\u001b[38;5;28mself\u001b[39m.optimizer, metrics=kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapproximator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28mself\u001b[39m._on_training_finished()\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py:315\u001b[39m, in \u001b[36mContinuousApproximator.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    264\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m \u001b[33;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\approximator.py:139\u001b[39m, in \u001b[36mApproximator.fit\u001b[39m\u001b[34m(self, dataset, simulator, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m     mock_data_shapes = keras.tree.map_structure(keras.ops.shape, mock_data)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.build(mock_data_shapes)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\backend_approximator.py:20\u001b[39m, in \u001b[36mBackendApproximator.fit\u001b[39m\u001b[34m(self, dataset, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, dataset: keras.utils.PyDataset, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfilter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:269\u001b[39m, in \u001b[36mTorchTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[32m    267\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[32m    272\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:124\u001b[39m, in \u001b[36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m    123\u001b[39m data = data[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\torch_approximator.py:101\u001b[39m, in \u001b[36mTorchApproximator.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m._update_metrics(metrics, \u001b[38;5;28mself\u001b[39m._batch_size_from_data(data))\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:526\u001b[39m, in \u001b[36mBaseOptimizer.apply\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    523\u001b[39m     grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:593\u001b[39m, in \u001b[36mBaseOptimizer._backend_apply_gradients\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28mself\u001b[39m._apply_weight_decay(trainable_variables)\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m._backend_update_step(\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m         grads, trainable_variables, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m    594\u001b[39m     )\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_ema:\n\u001b[32m    597\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_model_variables_moving_average(\n\u001b[32m    598\u001b[39m         \u001b[38;5;28mself\u001b[39m._trainable_variables\n\u001b[32m    599\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:712\u001b[39m, in \u001b[36mBaseOptimizer.learning_rate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearning_rate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_current_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:789\u001b[39m, in \u001b[36mBaseOptimizer._get_current_learning_rate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_current_learning_rate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    787\u001b[39m         \u001b[38;5;28mself\u001b[39m._learning_rate, learning_rate_schedule.LearningRateSchedule\n\u001b[32m    788\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._learning_rate, backend.Variable):\n\u001b[32m    791\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._learning_rate\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\optimizers\\schedules\\learning_rate_schedule.py:164\u001b[39m, in \u001b[36mExponentialDecay.__call__\u001b[39m\u001b[34m(self, step)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, step):\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(\u001b[38;5;28mself\u001b[39m.name):\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         initial_learning_rate = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_learning_rate\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m         dtype = initial_learning_rate.dtype\n\u001b[32m    168\u001b[39m         decay_steps = ops.cast(\u001b[38;5;28mself\u001b[39m.decay_steps, dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\ops\\core.py:999\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(x, dtype, sparse, ragged)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[32m    998\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ConvertToTensor(dtype=dtype, sparse=sparse, ragged=ragged)(x)\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mragged\u001b[49m\u001b[43m=\u001b[49m\u001b[43mragged\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\core.py:212\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(x, dtype, sparse, ragged)\u001b[39m\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.as_tensor(x, dtype=torch.int32, device=get_device())\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_torch_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfloatx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# Convert to np in case of any array-like that is not list or tuple.\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run optimization (adjust n_trials based on compute budget)\n",
    "# Each trial takes ~5-10 minutes depending on architecture\n",
    "N_TRIALS = 100  # Adjust as needed\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True,\n",
    "    gc_after_trial=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimization complete!\")\n",
    "print(f\"Total trials: {len(study.trials)}\")\n",
    "print(f\"Pareto-optimal trials: {len(study.best_trials)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Visualize the Pareto front and extract the best configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best configurations from Pareto front\n",
    "best_configs = summarize_best_trials(study)\n",
    "display(best_configs)\n",
    "\n",
    "# Print the best configuration for each objective\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDED CONFIGURATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(best_configs) > 0:\n",
    "    # Best for calibration (lowest cal_error)\n",
    "    best_cal = best_configs.iloc[0]\n",
    "    print(f\"\\n📊 Best Calibration (trial {int(best_cal['trial'])}):\")\n",
    "    print(f\"   Cal error: {best_cal['cal_error']:.4f}\")\n",
    "    print(f\"   Params: {int(best_cal['param_count']):,}\")\n",
    "    \n",
    "    # Best for size (if different)\n",
    "    if \"param_count\" in best_configs.columns:\n",
    "        best_size = best_configs.sort_values(\"param_count\").iloc[0]\n",
    "        if best_size[\"trial\"] != best_cal[\"trial\"]:\n",
    "            print(f\"\\n📦 Smallest Model (trial {int(best_size['trial'])}):\")\n",
    "            print(f\"   Cal error: {best_size['cal_error']:.4f}\")\n",
    "            print(f\"   Params: {int(best_size['param_count']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization results\n",
    "from rctbp_bf_training.core.optimization import plot_optimization_results\n",
    "\n",
    "fig = plot_optimization_results(study)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Until Threshold (Optional)\n",
    "\n",
    "Train the best configuration repeatedly until it meets the calibration error threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for threshold-based training\n",
    "thresholds = QualityThresholds(\n",
    "    max_cal_error=0.05,\n",
    "    max_c2st_deviation=0.05,\n",
    "    max_coverage_error=0.05,\n",
    "    max_iterations=40,\n",
    ")\n",
    "\n",
    "# Extended validation grid for final evaluation\n",
    "final_conditions = create_validation_grid(extended=True)\n",
    "\n",
    "print(f\"Quality thresholds:\")\n",
    "print(f\"  Max calibration error: {thresholds.max_cal_error}\")\n",
    "print(f\"  Max C2ST deviation: {thresholds.max_c2st_deviation}\")\n",
    "print(f\"  Max coverage error: {thresholds.max_coverage_error}\")\n",
    "print(f\"  Max iterations: {thresholds.max_iterations}\")\n",
    "print(f\"\\nFinal validation grid: {len(final_conditions)} conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training functions using factory\n",
    "build_workflow_fn, train_fn, validate_fn = create_ancova_training_functions(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    validation_conditions=final_conditions,\n",
    "    rng=RNG,\n",
    ")\n",
    "\n",
    "print(\"Training functions created via factory for train_until_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters from Pareto front and train until threshold\n",
    "if len(best_configs) > 0:\n",
    "    selected = best_configs.iloc[0]  # Best calibration\n",
    "    trial_num = int(selected[\"trial\"])\n",
    "    trial = [t for t in study.trials if t.number == trial_num][0]\n",
    "    best_params = trial.params\n",
    "    \n",
    "    print(f\"Selected trial {trial_num} for threshold training\")\n",
    "    print(f\"  Calibration error: {selected['cal_error']:.4f}\")\n",
    "    print(f\"  Parameter count: {int(selected['param_count']):,}\")\n",
    "    \n",
    "    # Train until threshold using the package function\n",
    "    result = train_until_threshold(\n",
    "        build_workflow_fn=build_workflow_fn,\n",
    "        train_fn=train_fn,\n",
    "        validate_fn=validate_fn,\n",
    "        hyperparams=best_params,\n",
    "        thresholds=thresholds,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    if result[\"converged\"]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"SUCCESS! Quality thresholds met\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Iterations: {result['iterations']}\")\n",
    "        print(f\"Final scores:\")\n",
    "        for key, val in result[\"best_scores\"].items():\n",
    "            print(f\"  {key}: {val:.4f}\")\n",
    "        \n",
    "        best_workflow = result[\"workflow\"]\n",
    "    else:\n",
    "        print(f\"\\nWarning: Did not meet thresholds after {result['iterations']} iterations\")\n",
    "        print(f\"Best scores achieved:\")\n",
    "        for key, val in result[\"best_scores\"].items():\n",
    "            print(f\"  {key}: {val:.4f}\")\n",
    "        \n",
    "        best_workflow = result[\"workflow\"]  # Use best attempt\n",
    "else:\n",
    "    print(\"No trials completed. Run optimization first.\")\n",
    "    best_workflow = None\n",
    "    result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model with metadata\n",
    "from pathlib import Path\n",
    "from rctbp_bf_training.models.ancova.model import get_model_metadata, save_model_with_metadata\n",
    "\n",
    "if best_workflow is not None and result is not None:\n",
    "    # Get final workflow config from best_params\n",
    "    final_workflow_config = params_dict_to_workflow_config(best_params)\n",
    "    \n",
    "    # Create ANCOVAConfig with optimized settings\n",
    "    config_optimized = ANCOVAConfig(\n",
    "        prior=config.prior,\n",
    "        meta=config.meta,\n",
    "        workflow=final_workflow_config,\n",
    "    )\n",
    "    \n",
    "    # Save with metadata\n",
    "    metadata = get_model_metadata(\n",
    "        config=config_optimized,\n",
    "        validation_results={\n",
    "            \"converged\": result[\"converged\"],\n",
    "            \"iterations\": result[\"iterations\"],\n",
    "            \"scores\": result[\"best_scores\"],\n",
    "            \"param_count\": get_param_count(best_workflow.approximator),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    save_path = Path(\"checkpoints\") / \"ancova_cont_2arms_optimized\"\n",
    "    saved_path = save_model_with_metadata(best_workflow.approximator, save_path, metadata)\n",
    "    \n",
    "    print(f\"\\n✓ Model saved to: {saved_path}\")\n",
    "    print(f\"✓ Metadata saved to: {saved_path.with_suffix('.json')}\")\n",
    "else:\n",
    "    print(\"No model to save. Run optimization and threshold training first.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
