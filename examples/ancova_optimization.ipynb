{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for 2-Arm ANCOVA NPE Model\n",
    "\n",
    "This notebook performs **Bayesian Optimization** (Optuna) to find optimal neural network architecture for the 2-arm ANCOVA amortized posterior estimation model.\n",
    "\n",
    "## Objectives\n",
    "1. **Minimize calibration error** (mean absolute coverage error)\n",
    "2. **Minimize parameter count** (model size)\n",
    "\n",
    "This is a multi-objective optimization with Pareto-optimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif not os.environ.get(\"KERAS_BACKEND\"):\n    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n    \nfrom pathlib import Path\nimport importlib\n\nimport numpy as np\nnp.set_printoptions(suppress=True)\nRNG = np.random.default_rng(2025)\n\nfrom itertools import product\n\nimport keras\nimport bayesflow as bf\n\nimport matplotlib.pyplot as plt\n\n# Import generic infrastructure from the package\nfrom rctbp_bf_training.core.infrastructure import (\n    SummaryNetworkConfig,\n    InferenceNetworkConfig,\n    WorkflowConfig,\n    params_dict_to_workflow_config,\n    build_summary_network,\n    build_inference_network,\n)\n\n# Import ANCOVA-specific functions\nfrom rctbp_bf_training.models.ancova.model import (\n    ANCOVAConfig,\n    create_adapter,\n    create_simulator,\n    create_ancova_workflow_components,\n    get_ancova_adapter_spec,\n    create_validation_grid,\n    make_simulate_fn,\n    get_model_metadata,\n    save_model_with_metadata,\n)\nfrom rctbp_bf_training.core.utils import MovingAverageEarlyStopping\n\n# Create default configuration\nconfig = ANCOVAConfig()\nprint(f\"Config loaded: {config.to_dict()['model_type'] if 'model_type' in config.to_dict() else 'ancova_cont_2arms'}\")\nprint(f\"\\nNetwork configs:\")\nprint(f\"  Summary network: {config.workflow.summary_network}\")\nprint(f\"  Inference network: {config.workflow.inference_network}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Define the simulator components: prior, likelihood, meta function, and adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulator and adapter using factory functions\n",
    "simulator = create_simulator(config, RNG)\n",
    "adapter = create_adapter()\n",
    "\n",
    "# Test\n",
    "sim_draws = simulator.sample(100)\n",
    "print(\"Simulator + Adapter created via factory functions\")\n",
    "print(f\"  sim keys: {list(sim_draws.keys())}\")\n",
    "print(f\"  N={sim_draws['N']}, p_alloc={sim_draws['p_alloc']:.2f}\")\n",
    "\n",
    "processed = adapter(sim_draws)\n",
    "print(f\"  inference_variables: {processed['inference_variables'].shape}\")\n",
    "print(f\"  summary_variables: {processed['summary_variables'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Decoupled Architecture Demo\n",
    "\n",
    "The refactored codebase decouples summary and inference networks for independent configuration and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation functions - all imported from the package\n",
    "from rctbp_bf_training.core import validation as functions_validation\n",
    "importlib.reload(functions_validation)\n",
    "\n",
    "from rctbp_bf_training.core.validation import (\n",
    "    run_validation_pipeline,\n",
    "    extract_calibration_metrics,\n",
    "    make_bayesflow_infer_fn,\n",
    ")\n",
    "\n",
    "# MovingAverageEarlyStopping imported from rctbp_bf_training.core.utils (in cell-1)\n",
    "# make_simulate_fn imported from rctbp_bf_training.models.ancova.model (in cell-1)\n",
    "\n",
    "print(\"Validation functions loaded:\")\n",
    "print(\"  - MovingAverageEarlyStopping from rctbp_bf_training.core.utils\")\n",
    "print(\"  - make_simulate_fn from rctbp_bf_training.models.ancova.model\")\n",
    "print(\"  - run_validation_pipeline from rctbp_bf_training.core.validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "Import BO infrastructure and define the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validation functions - all imported from the package\nfrom rctbp_bf_training.core import validation\nfrom rctbp_bf_training.core.validation import (\n    run_validation_pipeline,\n    extract_calibration_metrics,\n    make_bayesflow_infer_fn,\n)\n\nprint(\"Validation functions loaded from:\")\nprint(\"  - MovingAverageEarlyStopping from rctbp_bf_training.core.utils\")\nprint(\"  - make_simulate_fn from rctbp_bf_training.models.ancova.model\")\nprint(\"  - run_validation_pipeline from rctbp_bf_training.core.validation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Space and Optimization Grid\n",
    "\n",
    "Define the hyperparameter search space and a reduced validation grid for faster optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import BO infrastructure from the package\nfrom rctbp_bf_training.core import optimization\nfrom rctbp_bf_training.core.optimization import (\n    create_study,\n    sample_hyperparameters,\n    HyperparameterSpace,\n    get_param_count,\n    extract_objective_values,\n    cleanup_trial,\n    plot_optimization_results,\n    summarize_best_trials,\n)\n\nprint(\"Bayesian optimization infrastructure loaded from package\")\nprint(f\"Optuna available: {optimization.OPTUNA_AVAILABLE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The objective function builds a model with trial hyperparameters, trains it, validates on the reduced grid, and returns (calibration_error, param_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective: returns (calibration_error, param_count).\n",
    "    \n",
    "    Uses new decoupled API for building networks.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    params = sample_hyperparameters(trial, search_space)\n",
    "    \n",
    "    # NEW API: Convert params to WorkflowConfig with decoupled networks\n",
    "    workflow_config = params_dict_to_workflow_config(params)\n",
    "    \n",
    "    # NEW API: Build networks using decoupled configs\n",
    "    # This gives us independent summary and inference networks\n",
    "    summary_net = build_summary_network(workflow_config.summary_network)\n",
    "    inference_net = build_inference_network(workflow_config.inference_network)\n",
    "    \n",
    "    # Or equivalently, use the ANCOVA factory (returns summary_net, inference_net, adapter):\n",
    "    # ancova_config = ANCOVAConfig(workflow=workflow_config)\n",
    "    # summary_net, inference_net, adapter = create_ancova_workflow_components(ancova_config)\n",
    "    \n",
    "    # Setup learning rate schedule\n",
    "    steps_per_epoch = params[\"batch_size\"] * 100\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=params[\"initial_lr\"],\n",
    "        decay_steps=steps_per_epoch,\n",
    "        decay_rate=params[\"decay_rate\"],\n",
    "        staircase=True,\n",
    "    )\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    # Create workflow\n",
    "    wf = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=summary_net,\n",
    "        optimizer=opt,\n",
    "        inference_conditions=[\"N\", \"p_alloc\", \"prior_df\", \"prior_scale\"],\n",
    "        checkpoint_name=f\"optuna_trial_{trial.number}\",\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        wf.approximator.compile(optimizer=opt)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    early_stop = MovingAverageEarlyStopping(\n",
    "        window=params[\"window\"],\n",
    "        patience=params[\"patience\"],\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    try:\n",
    "        history = wf.fit_online(\n",
    "            epochs=config.workflow.training.epochs,\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            num_batches_per_epoch=config.workflow.training.batches_per_epoch,\n",
    "            validation_data=config.workflow.training.validation_sims,\n",
    "            callbacks=[early_stop],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} FAILED: {e}\")\n",
    "        cleanup_trial()\n",
    "        return 1.0, 1e9\n",
    "    \n",
    "    param_count = get_param_count(wf.approximator)\n",
    "    \n",
    "    # Validate\n",
    "    simulate_fn_opt = make_simulate_fn(rng=RNG)\n",
    "    infer_fn_opt = make_bayesflow_infer_fn(\n",
    "        wf.approximator,\n",
    "        param_key=\"b_group\",\n",
    "        data_keys=[\"outcome\", \"covariate\", \"group\"],\n",
    "        context_keys={\"N\": int, \"p_alloc\": float, \"prior_df\": float, \"prior_scale\": float},\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = run_validation_pipeline(\n",
    "            conditions_list=opt_conditions,\n",
    "            n_sims=500,\n",
    "            n_post_draws=500,\n",
    "            simulate_fn=simulate_fn_opt,\n",
    "            infer_fn=infer_fn_opt,\n",
    "            true_param_key=\"b_arm_treat\",\n",
    "            verbose=False,\n",
    "        )\n",
    "        cal_error, _ = extract_objective_values(results[\"metrics\"], param_count)\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} validation FAILED: {e}\")\n",
    "        cal_error = 1.0\n",
    "    \n",
    "    print(f\"Trial {trial.number}: cal_error={cal_error:.4f}, params={param_count:,}\")\n",
    "    \n",
    "    cleanup_trial()\n",
    "    del wf, summary_net, inference_net\n",
    "    gc.collect()\n",
    "    \n",
    "    return cal_error, param_count\n",
    "\n",
    "# Functions already imported in cell-1\n",
    "print(\"Objective function defined using NEW DECOUPLED API\")\n",
    "print(\"  - params_dict_to_workflow_config() converts hyperparameters\")\n",
    "print(\"  - build_summary_network() builds summary net independently\")\n",
    "print(\"  - build_inference_network() builds inference net independently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optimization\n",
    "\n",
    "Create and run the multi-objective Optuna study. Results are saved to SQLite for resumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def objective(trial):\n    \"\"\"\n    Optuna objective: returns (calibration_error, param_count).\n    \n    Uses decoupled API for building networks.\n    \"\"\"\n    import gc\n    \n    # Sample hyperparameters\n    params = sample_hyperparameters(trial, search_space)\n    \n    # Convert params to WorkflowConfig with decoupled networks\n    workflow_config = params_dict_to_workflow_config(params)\n    \n    # Build networks using decoupled configs\n    summary_net = build_summary_network(workflow_config.summary_network)\n    inference_net = build_inference_network(workflow_config.inference_network)\n    \n    # Setup learning rate schedule\n    steps_per_epoch = params[\"batch_size\"] * 100\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=params[\"initial_lr\"],\n        decay_steps=steps_per_epoch,\n        decay_rate=params[\"decay_rate\"],\n        staircase=True,\n    )\n    opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n    \n    # Create workflow\n    wf = bf.BasicWorkflow(\n        simulator=simulator,\n        adapter=adapter,\n        inference_network=inference_net,\n        summary_network=summary_net,\n        optimizer=opt,\n        inference_conditions=[\"N\", \"p_alloc\", \"prior_df\", \"prior_scale\"],\n        checkpoint_name=f\"optuna_trial_{trial.number}\",\n    )\n    \n    try:\n        wf.approximator.compile(optimizer=opt)\n    except Exception:\n        pass\n    \n    early_stop = MovingAverageEarlyStopping(\n        window=params[\"window\"],\n        patience=params[\"patience\"],\n        restore_best_weights=True,\n    )\n    \n    # Train\n    try:\n        history = wf.fit_online(\n            epochs=config.workflow.training.epochs,\n            batch_size=params[\"batch_size\"],\n            num_batches_per_epoch=config.workflow.training.batches_per_epoch,\n            validation_data=config.workflow.training.validation_sims,\n            callbacks=[early_stop],\n        )\n    except Exception as e:\n        print(f\"Trial {trial.number} FAILED: {e}\")\n        cleanup_trial()\n        return 1.0, 1e9\n    \n    param_count = get_param_count(wf.approximator)\n    \n    # Validate\n    simulate_fn_opt = make_simulate_fn(rng=RNG)\n    infer_fn_opt = make_bayesflow_infer_fn(\n        wf.approximator,\n        param_key=\"b_group\",\n        data_keys=[\"outcome\", \"covariate\", \"group\"],\n        context_keys={\"N\": int, \"p_alloc\": float, \"prior_df\": float, \"prior_scale\": float},\n    )\n    \n    try:\n        results = run_validation_pipeline(\n            conditions_list=opt_conditions,\n            n_sims=500,\n            n_post_draws=500,\n            simulate_fn=simulate_fn_opt,\n            infer_fn=infer_fn_opt,\n            true_param_key=\"b_arm_treat\",\n            verbose=False,\n        )\n        cal_error, _ = extract_objective_values(results[\"metrics\"], param_count)\n    except Exception as e:\n        print(f\"Trial {trial.number} validation FAILED: {e}\")\n        cal_error = 1.0\n    \n    print(f\"Trial {trial.number}: cal_error={cal_error:.4f}, params={param_count:,}\")\n    \n    cleanup_trial()\n    del wf, summary_net, inference_net\n    gc.collect()\n    \n    return cal_error, param_count\n\nprint(\"Objective function defined using decoupled API\")\nprint(\"  - params_dict_to_workflow_config() converts hyperparameters\")\nprint(\"  - build_summary_network() builds summary net independently\")\nprint(\"  - build_inference_network() builds inference net independently\")"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38625bf692e34481bbb501afc6f80fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OnlineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 1.6672 - val_loss: -0.7330 - moving_avg_val_loss: -0.7330\n",
      "Epoch 2/200\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 3s/step - loss: 4.4166 - val_loss: 0.3091 - moving_avg_val_loss: -0.2119\n",
      "Epoch 3/200\n",
      "\u001b[1m17/50\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 5s/step - loss: -0.0126[W 2025-12-22 21:36:05,402] Trial 0 failed with parameters: {'summary_dim': 8, 'deepset_width': 128, 'deepset_depth': 3, 'deepset_dropout': 0.31939631788866646, 'flow_depth': 3, 'flow_hidden': 48, 'flow_dropout': 0.07613762547568977, 'initial_lr': 0.002176624112345368, 'batch_size': 320} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Matze\\AppData\\Local\\Temp\\ipykernel_35916\\3257826582.py\", line 58, in objective\n",
      "    history = wf.fit_online(\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py\", line 789, in fit_online\n",
      "    return self._fit(\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py\", line 966, in _fit\n",
      "    self.history = self.approximator.fit(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py\", line 315, in fit\n",
      "    return super().fit(*args, **kwargs, adapter=self.adapter)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\approximator.py\", line 139, in fit\n",
      "    return super().fit(dataset=dataset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\backend_approximator.py\", line 20, in fit\n",
      "    return super().fit(x=dataset, y=None, **filter_kwargs(kwargs, super().fit))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py\", line 269, in fit\n",
      "    logs = self.train_function(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py\", line 124, in one_step_on_data\n",
      "    return self.train_step(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\torch_approximator.py\", line 88, in train_step\n",
      "    metrics = self.compute_metrics(**kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py\", line 222, in compute_metrics\n",
      "    inference_variables = self._prepare_inference_variables(inference_variables, stage=stage)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py\", line 259, in _prepare_inference_variables\n",
      "    inference_variables = self.standardize_layers[\"inference_variables\"](inference_variables, stage=stage)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 941, in __call__\n",
      "    outputs = super().__call__(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\layer.py\", line 41, in forward\n",
      "    return Operation.__call__(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 59, in __call__\n",
      "    return call_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\networks\\standardization\\standardization.py\", line 101, in call\n",
      "    self._update_moments(val, idx)\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\networks\\standardization\\standardization.py\", line 156, in _update_moments\n",
      "    batch_count = keras.ops.cast(keras.ops.prod(keras.ops.shape(x)[:-1]), self.count[index].dtype)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 5369, in prod\n",
      "    return backend.numpy.prod(x, axis=axis, keepdims=keepdims, dtype=dtype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\numpy.py\", line 1337, in prod\n",
      "    x = convert_to_tensor(x)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\core.py\", line 236, in convert_to_tensor\n",
      "    return torch.as_tensor(x, dtype=dtype, device=get_device())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-12-22 21:36:05,414] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run optimization (adjust n_trials based on compute budget)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Each trial takes ~5-10 minutes depending on architecture\u001b[39;00m\n\u001b[32m      3\u001b[39m N_TRIALS = \u001b[32m30\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptimization complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal trials: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study.trials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     history = \u001b[43mwf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_batches_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatches_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_sims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m FAILED: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py:789\u001b[39m, in \u001b[36mBasicWorkflow.fit_online\u001b[39m\u001b[34m(self, epochs, num_batches_per_epoch, batch_size, keep_optimizer, validation_data, augmentations, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    742\u001b[39m \u001b[33;03mTrain the approximator using an online data-generating process. The dataset is dynamically generated during\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[33;03mtraining, making this approach suitable for scenarios where generating new simulations is computationally cheap.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    778\u001b[39m \u001b[33;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m dataset = OnlineDataset(\n\u001b[32m    782\u001b[39m     simulator=\u001b[38;5;28mself\u001b[39m.simulator,\n\u001b[32m    783\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     augmentations=augmentations,\n\u001b[32m    787\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43monline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\workflows\\basic_workflow.py:966\u001b[39m, in \u001b[36mBasicWorkflow._fit\u001b[39m\u001b[34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[39m\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mself\u001b[39m.approximator.compile(optimizer=\u001b[38;5;28mself\u001b[39m.optimizer, metrics=kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapproximator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28mself\u001b[39m._on_training_finished()\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py:315\u001b[39m, in \u001b[36mContinuousApproximator.fit\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    264\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m \u001b[33;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\approximator.py:139\u001b[39m, in \u001b[36mApproximator.fit\u001b[39m\u001b[34m(self, dataset, simulator, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m     mock_data_shapes = keras.tree.map_structure(keras.ops.shape, mock_data)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.build(mock_data_shapes)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\backend_approximator.py:20\u001b[39m, in \u001b[36mBackendApproximator.fit\u001b[39m\u001b[34m(self, dataset, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, dataset: keras.utils.PyDataset, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfilter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:269\u001b[39m, in \u001b[36mTorchTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[32m    267\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[32m    272\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\trainer.py:124\u001b[39m, in \u001b[36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m    123\u001b[39m data = data[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\backend_approximators\\torch_approximator.py:88\u001b[39m, in \u001b[36mTorchApproximator.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m     87\u001b[39m     kwargs = filter_kwargs(data | {\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m}, \u001b[38;5;28mself\u001b[39m.compute_metrics)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m loss = metrics[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# noinspection PyUnresolvedReferences\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py:222\u001b[39m, in \u001b[36mContinuousApproximator.compute_metrics\u001b[39m\u001b[34m(self, inference_variables, inference_conditions, summary_variables, sample_weight, stage)\u001b[39m\n\u001b[32m    219\u001b[39m     inference_conditions = \u001b[38;5;28mself\u001b[39m.standardize_layers[\u001b[33m\"\u001b[39m\u001b[33minference_conditions\u001b[39m\u001b[33m\"\u001b[39m](inference_conditions, stage=stage)\n\u001b[32m    220\u001b[39m inference_conditions = concatenate_valid((inference_conditions, summary_outputs), axis=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m inference_variables = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_inference_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m inference_metrics = \u001b[38;5;28mself\u001b[39m.inference_network.compute_metrics(\n\u001b[32m    225\u001b[39m     inference_variables, conditions=inference_conditions, sample_weight=sample_weight, stage=stage\n\u001b[32m    226\u001b[39m )\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m summary_metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\approximators\\continuous_approximator.py:259\u001b[39m, in \u001b[36mContinuousApproximator._prepare_inference_variables\u001b[39m\u001b[34m(self, inference_variables, stage)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Helper function to convert inference variables to tensors and optionally standardize them.\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minference_variables\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.standardize:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     inference_variables = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstandardize_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minference_variables\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inference_variables\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:941\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    939\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    945\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\layer.py:41\u001b[39m, in \u001b[36mTorchLayer.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\networks\\standardization\\standardization.py:101\u001b[39m, in \u001b[36mStandardization.call\u001b[39m\u001b[34m(self, x, stage, forward, log_det_jac, transformation_type, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(flattened):\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stage == \u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_moments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     mean = expand_left_as(\u001b[38;5;28mself\u001b[39m.moving_mean[idx], val)\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# moving_std will return 1 in the case of std=0, so no further checks are necessary here\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\bayesflow\\networks\\standardization\\standardization.py:156\u001b[39m, in \u001b[36mStandardization._update_moments\u001b[39m\u001b[34m(self, x, index)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mIncrementally updates the running mean and variance (M2) per feature using a numerically\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03mstable online algorithm.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m \u001b[33;03m    The index of the corresponding running statistics to be updated.\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    155\u001b[39m reduce_axes = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(x.ndim - \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m batch_count = keras.ops.cast(\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.count[index].dtype)\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Compute batch mean and M2 per feature\u001b[39;00m\n\u001b[32m    159\u001b[39m batch_mean = keras.ops.mean(x, axis=reduce_axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\ops\\numpy.py:5369\u001b[39m, in \u001b[36mprod\u001b[39m\u001b[34m(x, axis, keepdims, dtype)\u001b[39m\n\u001b[32m   5367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[32m   5368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Prod(axis=axis, keepdims=keepdims, dtype=dtype).symbolic_call(x)\n\u001b[32m-> \u001b[39m\u001b[32m5369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\numpy.py:1337\u001b[39m, in \u001b[36mprod\u001b[39m\u001b[34m(x, axis, keepdims, dtype)\u001b[39m\n\u001b[32m   1336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprod\u001b[39m(x, axis=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1337\u001b[39m     x = \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1339\u001b[39m         dtype = dtypes.result_type(x.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matze\\Documents\\GitHub\\rctbp_bf_training\\venv-py312\\Lib\\site-packages\\keras\\src\\backend\\torch\\core.py:236\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(x, dtype, sparse, ragged)\u001b[39m\n\u001b[32m    232\u001b[39m     dtype = result_type(\n\u001b[32m    233\u001b[39m         *[\u001b[38;5;28mgetattr\u001b[39m(item, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(item)) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tree.flatten(x)]\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    235\u001b[39m dtype = to_torch_dtype(dtype)\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run optimization (adjust n_trials based on compute budget)\n",
    "# Each trial takes ~5-10 minutes depending on architecture\n",
    "N_TRIALS = 30  # Adjust as needed\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True,\n",
    "    gc_after_trial=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimization complete!\")\n",
    "print(f\"Total trials: {len(study.trials)}\")\n",
    "print(f\"Pareto-optimal trials: {len(study.best_trials)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Visualize the Pareto front and extract the best configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization results\n",
    "importlib.reload(bo)\n",
    "from rctbp_bf_training.core.optimization import plot_optimization_results\n",
    "\n",
    "fig = plot_optimization_results(study)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best configurations from Pareto front\n",
    "best_configs = summarize_best_trials(study)\n",
    "display(best_configs)\n",
    "\n",
    "# Print the best configuration for each objective\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDED CONFIGURATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(best_configs) > 0:\n",
    "    # Best for calibration (lowest cal_error)\n",
    "    best_cal = best_configs.iloc[0]\n",
    "    print(f\"\\n📊 Best Calibration (trial {int(best_cal['trial'])}):\")\n",
    "    print(f\"   Cal error: {best_cal['cal_error']:.4f}\")\n",
    "    print(f\"   Params: {int(best_cal['param_count']):,}\")\n",
    "    \n",
    "    # Best for size (if different)\n",
    "    if \"param_count\" in best_configs.columns:\n",
    "        best_size = best_configs.sort_values(\"param_count\").iloc[0]\n",
    "        if best_size[\"trial\"] != best_cal[\"trial\"]:\n",
    "            print(f\"\\n📦 Smallest Model (trial {int(best_size['trial'])}):\")\n",
    "            print(f\"   Cal error: {best_size['cal_error']:.4f}\")\n",
    "            print(f\"   Params: {int(best_size['param_count']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Best Configuration\n",
    "\n",
    "Copy the best hyperparameters to the main configuration cells above to retrain with the optimal architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which Pareto-optimal trial to use\n",
    "# Options: choose by index (0 = best calibration) or by trial number\n",
    "SELECTED_TRIAL_IDX = 0  # Index in best_configs DataFrame\n",
    "\n",
    "if len(best_configs) > 0:\n",
    "    selected = best_configs.iloc[SELECTED_TRIAL_IDX]\n",
    "    trial_num = int(selected[\"trial\"])\n",
    "    \n",
    "    # Get full trial parameters\n",
    "    trial = [t for t in study.trials if t.number == trial_num][0]\n",
    "    params = trial.params\n",
    "    \n",
    "    print(f\"Selected Trial {trial_num}\")\n",
    "    print(f\"Calibration Error: {selected['cal_error']:.4f}\")\n",
    "    print(f\"Parameter Count: {int(selected['param_count']):,}\")\n",
    "    print(\"\\nHyperparameters to use:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Print as copy-pasteable configuration\n",
    "    print(f\"\"\"\n",
    "# Copy these values to the HYPERPARAMETERS cell above:\n",
    "SUMMARY_DIM = {params['summary_dim']}\n",
    "DEEPSET_DEPTH = {params['deepset_depth']}\n",
    "DEEPSET_WIDTH = {params['deepset_width']}\n",
    "DEEPSET_DROPOUT = {params['deepset_dropout']:.4f}\n",
    "\n",
    "FLOW_DEPTH = {params['flow_depth']}\n",
    "FLOW_HIDDEN = {params['flow_hidden']}\n",
    "FLOW_DROPOUT = {params['flow_dropout']:.4f}\n",
    "\n",
    "INITIAL_LR = {params['initial_lr']:.6f}\n",
    "BATCH_SIZE = {params['batch_size']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Until Threshold\n",
    "\n",
    "Train the best configuration repeatedly until it meets the calibration error threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for threshold-based training\n",
    "CAL_ERROR_THRESHOLD = 0.05\n",
    "MAX_ATTEMPTS = 40\n",
    "FULL_EPOCHS = 50\n",
    "FULL_BATCHES = 100\n",
    "\n",
    "# Use factory function for extended validation grid\n",
    "final_conditions = create_validation_grid(extended=True)\n",
    "\n",
    "print(f\"Threshold: {CAL_ERROR_THRESHOLD}\")\n",
    "print(f\"Max attempts: {MAX_ATTEMPTS}\")\n",
    "print(f\"Final validation grid: {len(final_conditions)} conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until_threshold(params, threshold, max_attempts, epochs=50, batches_per_epoch=100):\n",
    "    \"\"\"\n",
    "    Train until calibration threshold is met.\n",
    "    \n",
    "    Uses new decoupled API for network building.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    \n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        print(f\"\\n{'='*60}\\nATTEMPT {attempt}/{max_attempts}\\n{'='*60}\")\n",
    "        \n",
    "        # NEW API: Convert params to WorkflowConfig\n",
    "        workflow_config = params_dict_to_workflow_config(params)\n",
    "        \n",
    "        # NEW API: Build decoupled networks\n",
    "        summary_net = build_summary_network(workflow_config.summary_network)\n",
    "        inference_net = build_inference_network(workflow_config.inference_network)\n",
    "        \n",
    "        print(f\"Networks built:\")\n",
    "        print(f\"  Summary: dim={workflow_config.summary_network.summary_dim}, \"\n",
    "              f\"depth={workflow_config.summary_network.depth}, \"\n",
    "              f\"width={workflow_config.summary_network.width}\")\n",
    "        print(f\"  Inference: depth={workflow_config.inference_network.depth}, \"\n",
    "              f\"hidden={workflow_config.inference_network.hidden_sizes}\")\n",
    "        \n",
    "        # Setup optimizer\n",
    "        steps_per_epoch = params[\"batch_size\"] * batches_per_epoch\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=params[\"initial_lr\"],\n",
    "            decay_steps=steps_per_epoch,\n",
    "            decay_rate=params.get(\"decay_rate\", 0.85),\n",
    "            staircase=True,\n",
    "        )\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        # Create workflow\n",
    "        wf = bf.BasicWorkflow(\n",
    "            simulator=simulator,\n",
    "            adapter=adapter,\n",
    "            inference_network=inference_net,\n",
    "            summary_network=summary_net,\n",
    "            optimizer=opt,\n",
    "            inference_conditions=[\"N\", \"p_alloc\", \"prior_df\", \"prior_scale\"],\n",
    "            checkpoint_name=f\"best_model_attempt_{attempt}\",\n",
    "        )\n",
    "        \n",
    "        early_stop = MovingAverageEarlyStopping(\n",
    "            window=params.get(\"window\", 10),\n",
    "            patience=params.get(\"patience\", 10),\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        try:\n",
    "            wf.fit_online(\n",
    "                epochs=epochs,\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                num_batches_per_epoch=batches_per_epoch,\n",
    "                validation_data=config.workflow.training.validation_sims,\n",
    "                callbacks=[early_stop],\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            del wf, summary_net, inference_net\n",
    "            gc.collect()\n",
    "            continue\n",
    "        \n",
    "        # Validate using imported factory functions\n",
    "        simulate_fn = make_simulate_fn(rng=RNG)\n",
    "        infer_fn = make_bayesflow_infer_fn(\n",
    "            wf.approximator,\n",
    "            param_key=\"b_group\",\n",
    "            data_keys=[\"outcome\", \"covariate\", \"group\"],\n",
    "            context_keys={\"N\": int, \"p_alloc\": float, \"prior_df\": float, \"prior_scale\": float},\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            results = run_validation_pipeline(\n",
    "                conditions_list=final_conditions,\n",
    "                n_sims=1000,\n",
    "                n_post_draws=1000,\n",
    "                simulate_fn=simulate_fn,\n",
    "                infer_fn=infer_fn,\n",
    "                true_param_key=\"b_arm_treat\",\n",
    "                verbose=True,\n",
    "            )\n",
    "            param_count = get_param_count(wf.approximator)\n",
    "            cal_error, _ = extract_objective_values(results[\"metrics\"], param_count)\n",
    "        except Exception as e:\n",
    "            print(f\"Validation failed: {e}\")\n",
    "            del wf, summary_net, inference_net\n",
    "            gc.collect()\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nAttempt {attempt}: cal_error={cal_error:.4f}, threshold={threshold:.4f}\")\n",
    "        \n",
    "        if cal_error <= threshold:\n",
    "            print(\"✓ SUCCESS! Threshold met.\")\n",
    "            return wf, cal_error, attempt, workflow_config\n",
    "        \n",
    "        del wf, summary_net, inference_net\n",
    "        gc.collect()\n",
    "    \n",
    "    return None, None, None, None\n",
    "\n",
    "print(\"train_until_threshold defined (using new decoupled API)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model until threshold is met\n",
    "best_workflow, final_cal_error, successful_attempt, best_workflow_config = train_until_threshold(\n",
    "    params=params,  # From the \"Apply Best Configuration\" cell\n",
    "    threshold=CAL_ERROR_THRESHOLD,\n",
    "    max_attempts=MAX_ATTEMPTS,\n",
    "    epochs=FULL_EPOCHS,\n",
    "    batches_per_epoch=FULL_BATCHES,\n",
    ")\n",
    "\n",
    "if best_workflow is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL MODEL READY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Achieved calibration error: {final_cal_error:.4f}\")\n",
    "    print(f\"Successful on attempt: {successful_attempt}\")\n",
    "    print(f\"Model parameters: {get_param_count(best_workflow.approximator):,}\")\n",
    "    print(f\"\\nFinal network configurations:\")\n",
    "    print(f\"  Summary: {best_workflow_config.summary_network}\")\n",
    "    print(f\"  Inference: {best_workflow_config.inference_network}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with metadata using new API\n",
    "if best_workflow is not None:\n",
    "    # Create ANCOVAConfig with the optimized workflow config\n",
    "    config_with_optimized = ANCOVAConfig(\n",
    "        prior=config.prior,\n",
    "        meta=config.meta,\n",
    "        workflow=best_workflow_config,  # Use the optimized workflow config\n",
    "    )\n",
    "    \n",
    "    # Get metadata using new infrastructure\n",
    "    metadata = get_model_metadata(\n",
    "        config=config_with_optimized,\n",
    "        validation_results={\n",
    "            \"calibration_error\": final_cal_error,\n",
    "            \"successful_attempt\": successful_attempt,\n",
    "            \"param_count\": get_param_count(best_workflow.approximator),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Save using infrastructure's save function\n",
    "    save_path = Path(\"checkpoints\") / \"ancova_cont_2arms_optimized\"\n",
    "    saved_path = save_model_with_metadata(best_workflow.approximator, save_path, metadata)\n",
    "    \n",
    "    print(f\"✓ Model saved to: {saved_path}\")\n",
    "    print(f\"✓ Metadata saved to: {saved_path.with_suffix('.json')}\")\n",
    "    print(f\"\\nSaved configuration:\")\n",
    "    print(f\"  Summary network: dim={best_workflow_config.summary_network.summary_dim}, \"\n",
    "          f\"depth={best_workflow_config.summary_network.depth}\")\n",
    "    print(f\"  Inference network: depth={best_workflow_config.inference_network.depth}, \"\n",
    "          f\"hidden={best_workflow_config.inference_network.hidden_sizes}\")\n",
    "else:\n",
    "    print(\"No model to save\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}