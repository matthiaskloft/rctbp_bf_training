{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Arm ANCOVA Model for Continuous Outcomes (No Intercept, Fixed Error Variance)\n",
    "\n",
    "## Model Specification\n",
    "\n",
    "This notebook implements a 2-arm ANCOVA model **without an intercept** and with **fixed error variance**:\n",
    "\n",
    "$$\\text{outcome} = b_1 \\cdot \\text{covariate} + b_2 \\cdot \\text{group} + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $b_1$: coefficient for the baseline covariate **(NOT estimated - fixed in simulation)**\n",
    "- $b_2$: treatment effect (difference between groups) **(parameter to estimate via BayesFlow)**\n",
    "- $\\text{group}$: binary indicator (0 = control, 1 = treatment)\n",
    "- $\\epsilon \\sim N(0, 1)$: residual error with **FIXED variance = 1**\n",
    "\n",
    "**Key features:**\n",
    "- No intercept term ($\\beta_0 = 0$)\n",
    "- Only 1 parameter estimated: $b_2$ (treatment effect)\n",
    "- $b_1$ (covariate effect) is simulated but not inferred\n",
    "- Error variance $\\sigma^2 = 1$ (not estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif not os.environ.get(\"KERAS_BACKEND\"):\n    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n    \nfrom pathlib import Path\nimport importlib\n\nimport numpy as np\nnp.set_printoptions(suppress=True)\nRNG = np.random.default_rng(2025)\n\nfrom itertools import product\n\nimport keras\nimport bayesflow as bf\n\n# plotting\nimport matplotlib.pyplot as plt\n\n# Import ANCOVA-specific functions and configs from the package\nfrom rctbp_bf_training.models.ancova.model import (\n    ANCOVAConfig,\n    PriorConfig,\n    MetaConfig,\n    NetworkConfig,\n    TrainingConfig,\n    create_adapter,\n    create_simulator,\n    build_networks,\n    create_validation_grid,\n    make_simulate_fn,\n    make_infer_fn,\n    simulate_cond_batch,\n    get_model_metadata,\n    save_model_with_metadata,\n    load_model_with_metadata,\n)\nfrom rctbp_bf_training.core.utils import MovingAverageEarlyStopping, loguniform_int\n\n# Create default configuration\nconfig = ANCOVAConfig()\nprint(f\"Config loaded. Model type: ancova_cont_2arms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Simulator Components\n\nThe simulator is built from three functions (defined in `ancova_cont_2arms_fns.py`):\n- **prior**: Samples `b_covariate` and `b_group` parameters\n- **likelihood**: Simulates ANCOVA data given parameters\n- **meta**: Samples context variables (N, p_alloc, prior_df, prior_scale)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prior, likelihood, meta functions are all in ancova_cont_2arms_fns\n# We use create_simulator() to build the complete simulator with config injection\nprint(\"Simulator functions defined in ancova_cont_2arms_fns:\")\nprint(\"  - prior(prior_df, prior_scale, config, rng)\")\nprint(\"  - likelihood(b_covariate, b_group, N, p_alloc, rng)\")\nprint(\"  - meta(config, rng)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create simulator using factory function (injects config and RNG)\nsimulator = create_simulator(config, RNG)\n\n# Test sampling\nsim_draws = simulator.sample(100)\nprint(\"Simulator output keys:\", sim_draws.keys())\nprint(f\"N: {sim_draws['N']}\")\nprint(f\"p_alloc: {sim_draws['p_alloc']:.3f}\")\nprint(f\"prior_df: {sim_draws['prior_df']}\")\nprint(f\"prior_scale: {sim_draws['prior_scale']:.3f}\")\nprint(f\"b_covariate shape: {sim_draws['b_covariate'].shape}\")\nprint(f\"b_group shape: {sim_draws['b_group'].shape}\")\nprint(f\"outcome[0] shape: {sim_draws['outcome'][0].shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Gamma distribution for prior_scale\n",
    "from scipy import stats\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Gamma(shape=2, scale=1) for prior_scale\n",
    "x_gamma = np.linspace(0, 8, 200)\n",
    "shape, scale = 2, 1\n",
    "gamma_pdf = stats.gamma.pdf(x_gamma, a=shape, scale=scale)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x_gamma, gamma_pdf, 'b-', lw=2)\n",
    "ax.fill_between(x_gamma, gamma_pdf, alpha=0.3)\n",
    "ax.axvline(shape * scale, color='r', ls='--', label=f'Mean = {shape * scale}')\n",
    "ax.axvline((shape - 1) * scale, color='g', ls='--', label=f'Mode = {(shape - 1) * scale}')\n",
    "ax.set_xlabel('prior_scale')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Gamma(shape={shape}, scale={scale}) for prior_scale')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 8)\n",
    "\n",
    "# Log-uniform distribution for prior_df (0 to 30)\n",
    "ax = axes[1]\n",
    "# Sample from the actual distribution\n",
    "samples_df = [int(round(loguniform_int(1, 31, alpha=0.7) - 1)) for _ in range(10000)]\n",
    "ax.hist(samples_df, bins=np.arange(-0.5, 31.5, 1), density=True, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('prior_df')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Log-uniform(1, 31, α=0.7) - 1 for prior_df')\n",
    "ax.set_xlim(-1, 31)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"prior_scale: Gamma(shape=2, scale=1) → mean={shape*scale}, mode={(shape-1)*scale}\")\n",
    "print(f\"prior_df: integer in [0, 30], log-uniform with α=0.7 (skewed toward lower values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter\n",
    "\n",
    "Transform simulator output for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Adapter created via factory function\nadapter = create_adapter()\n    \n# Test the adapter\nprocessed = adapter(sim_draws)\nprint(\"Processed data shapes:\")\nprint(\"  inference_variables:\", processed[\"inference_variables\"].shape)\nprint(\"  inference_conditions:\", processed[\"inference_conditions\"].shape)\nprint(\"  summary_variables:\", processed[\"summary_variables\"].shape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks and Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# HYPERPARAMETERS (from config, can be overridden)\n# =============================================================================\n# Use config defaults or override for experimentation\n\nnet_config = config.network\ntrain_config = config.training\n\n# Print current configuration\nprint(\"Network configuration:\")\nprint(f\"  DeepSet: dim={net_config.summary_dim}, depth={net_config.deepset_depth}, \" +\n      f\"width={net_config.deepset_width}, dropout={net_config.deepset_dropout}\")\nprint(f\"  Flow: depth={net_config.flow_depth}, hidden={net_config.flow_hidden}, \" +\n      f\"dropout={net_config.flow_dropout}\")\nprint(f\"\\nTraining configuration:\")\nprint(f\"  lr={train_config.initial_lr}, batch={train_config.batch_size}, \" +\n      f\"epochs={train_config.epochs}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build networks using factory function\nsummary_network, inference_network = build_networks(net_config)\n\nprint(f\"Networks created:\")\nprint(f\"  DeepSet: summary_dim={net_config.summary_dim}, depth={net_config.deepset_depth}\")\nprint(f\"  CouplingFlow: depth={net_config.flow_depth}, hidden={net_config.flow_hidden}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimizer with decaying learning rate\nsteps_per_epoch = train_config.batch_size * train_config.batches_per_epoch\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=train_config.initial_lr,\n    decay_steps=steps_per_epoch,\n    decay_rate=train_config.decay_rate,  \n    staircase=True,\n)\noptimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n\n# Create workflow\nworkflow = bf.BasicWorkflow(\n    simulator=simulator,\n    adapter=adapter,\n    inference_network=inference_network,\n    summary_network=summary_network,\n    optimizer=optimizer,\n    inference_conditions=[\"N\", \"p_alloc\", \"prior_df\", \"prior_scale\"],\n    checkpoint_name=\"ancova_cont_2arms\"\n)\n\nprint(\"Workflow created (model will be built on first forward pass)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train the approximator to learn the posterior distribution of `b_covariate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MovingAverageEarlyStopping imported from utils (see imports cell)\nprint(\"MovingAverageEarlyStopping available from utils\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "callback = MovingAverageEarlyStopping(\n    window=train_config.early_stopping_window, \n    patience=train_config.early_stopping_patience\n)\n\nhistory = workflow.fit_online(\n    epochs=train_config.epochs, \n    batch_size=train_config.batch_size,     \n    num_batches_per_epoch=train_config.batches_per_epoch, \n    validation_data=train_config.validation_sims,\n    callbacks=[callback]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plots.loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Validate the trained approximator with simulation-based calibration and recovery diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-in-One Diagnostic Plots\n",
    "\n",
    "Generate all standard diagnostic plots at once using the built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of posterior draws you want to get\n",
    "num_samples = 1000\n",
    "\n",
    "# Simulate validation data (unseen during training)\n",
    "val_sims = simulator.sample(1000)\n",
    "\n",
    "# Obtain num_samples samples of the parameter posterior for every validation dataset\n",
    "post_draws = workflow.sample(conditions=val_sims, num_samples=num_samples)\n",
    "\n",
    "# Compute metrics for the validation set\n",
    "metrics = workflow.compute_default_diagnostics(test_data=val_sims)\n",
    "metrics[\"b_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sims.keys())\n",
    "print(val_sims['N'])\n",
    "print(val_sims['p_alloc'])\n",
    "print(val_sims['group'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import plot functions from the package\nfrom rctbp_bf_training.plotting import diagnostics as plot\nimportlib.reload(plot)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Diagnostic Dashboard\n",
    "\n",
    "All calibration diagnostics in one comprehensive view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diagnostic dashboard using BayesFlow plots + coverage difference\n",
    "fig = plot.plot_diagnostic_dashboard(\n",
    "    estimates=post_draws,\n",
    "    targets=val_sims,\n",
    "    param_key=\"b_group\",\n",
    "    variable_name=r\"$b_2$ (treatment effect)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation on Conditions Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Function\n",
    "Function to simulate and fit one condition on a grid of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import generic validation functions from the package\nfrom rctbp_bf_training.core import validation as functions_validation\nimportlib.reload(functions_validation)\n\nfrom rctbp_bf_training.core.validation import (\n    run_validation_pipeline,\n    extract_calibration_metrics,\n    make_bayesflow_infer_fn,\n)\n\n# simulate_cond_batch and make_simulate_fn imported from rctbp_bf_training.models.ancova.model\nprint(\"Validation functions loaded from:\")\nprint(\"  - rctbp_bf_training.core.validation: run_validation_pipeline, make_bayesflow_infer_fn\")\nprint(\"  - rctbp_bf_training.models.ancova.model: simulate_cond_batch, make_simulate_fn\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges for the validation grid\n",
    "b_covariate_vals = [-0.5, 0.5]\n",
    "b_group_vals = [0.0, 0.5, 1.0]\n",
    "N_vals = [20, 1000]\n",
    "p_alloc_vals = [0.5]\n",
    "prior_df_vals = [0, 3]  # 0 = Normal, 10 = moderate t, 50 = nearly Normal\n",
    "prior_scale_vals = [0.1, 1, 10]\n",
    "\n",
    "# Build conditions grid with proper format for run_validation_pipeline\n",
    "conditions = []\n",
    "for idx, (n, p, b_cov, b_grp, pdf, psc) in enumerate(\n",
    "    product(N_vals, p_alloc_vals, b_covariate_vals, b_group_vals, prior_df_vals, prior_scale_vals)\n",
    "):\n",
    "    conditions.append({\n",
    "        \"id_cond\": idx,\n",
    "        \"n_total\": n,\n",
    "        \"p_alloc\": p,\n",
    "        \"b_covariate\": b_cov,\n",
    "        \"b_arm_treat\": b_grp,  # true treatment effect\n",
    "        \"prior_df\": pdf,\n",
    "        \"prior_scale\": psc,\n",
    "    })\n",
    "\n",
    "print(f\"Total conditions: {len(conditions)}\")\n",
    "print(f\"Example condition: {conditions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the trained workflow's approximator (or load from checkpoint)\n# model_path = \"checkpoints/ancova_cont_2arms.keras\"\n# model_ancova = keras.saving.load_model(model_path)\nmodel_ancova = workflow.approximator\n\n# Create adapters using imported factory functions\nsimulate_fn = make_simulate_fn(rng=RNG)\ninfer_fn = make_bayesflow_infer_fn(\n    model_ancova, \n    param_key=\"b_group\",\n    data_keys=[\"outcome\", \"covariate\", \"group\"],\n    context_keys={\"N\": int, \"p_alloc\": float, \"prior_df\": float, \"prior_scale\": float}\n)\n\n# Run the complete validation pipeline\nresults = run_validation_pipeline(\n    conditions_list=conditions,\n    n_sims=1000,\n    n_post_draws=1000,\n    simulate_fn=simulate_fn,\n    infer_fn=infer_fn,\n    true_param_key=\"b_arm_treat\",\n    verbose=True\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "metrics = results[\"metrics\"]\n",
    "metrics[\"condition_metrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "metrics = results[\"metrics\"]\n",
    "\n",
    "# Condition-level summary statistics\n",
    "display(metrics[\"condition_summary\"].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot comprehensive SBC diagnostics (4 panels: histogram, ECDF, coverage, recovery)\nfrom rctbp_bf_training.plotting import diagnostics as plot\nimportlib.reload(plot)\n\n# Works directly on the metrics dict - shows all conditions combined\nfig = plot.plot_sbc_diagnostics(metrics)\n\n# Print key SBC test statistics\nprint(f\"SBC KS p-value: {metrics['summary']['sbc_ks_pvalue']:.4f}\")\nprint(f\"SBC C2ST accuracy: {metrics['summary']['sbc_c2st_accuracy']:.4f} (0.5 = well-calibrated)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_plots as plot\n",
    "importlib.reload(plot)\n",
    "max_conditions=len(metrics['condition_metrics'])\n",
    "fig = plot.plot_ecdf_by_condition(metrics, max_conditions=max_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plot)\n",
    "max_conditions=len(metrics['condition_metrics'])\n",
    "fig = plot.plot_histogram_by_condition(metrics, max_conditions=max_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plot)\n",
    "max_conditions=len(metrics['condition_metrics'])\n",
    "fig = plot.plot_coverage_by_condition(metrics, max_conditions=max_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the trained model with metadata\nmetadata = get_model_metadata(config)\nsave_path = Path(\"checkpoints\") / \"ancova_cont_2arms\"\nsaved_path = save_model_with_metadata(workflow.approximator, save_path, metadata)\n\nprint(f\"Model saved to: {saved_path}\")\nprint(f\"Metadata saved to: {saved_path.with_suffix('.json')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model with metadata\nmodel_path = \"checkpoints/ancova_cont_2arms\"\nloaded_model, loaded_metadata = load_model_with_metadata(model_path)\n\nprint(\"Model loaded successfully\")\nif loaded_metadata:\n    print(f\"Created: {loaded_metadata.get('created_at', 'unknown')}\")\n    print(f\"Model type: {loaded_metadata.get('model_type', 'unknown')}\")"
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py311_sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}