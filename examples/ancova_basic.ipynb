{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Arm ANCOVA Model for Continuous Outcomes (No Intercept, Fixed Error Variance)\n",
    "\n",
    "## Model Specification\n",
    "\n",
    "This notebook implements a 2-arm ANCOVA model **without an intercept** and with **fixed error variance**:\n",
    "\n",
    "$$\\text{outcome} = b_1 \\cdot \\text{covariate} + b_2 \\cdot \\text{group} + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $b_1$: coefficient for the baseline covariate **(NOT estimated - fixed in simulation)**\n",
    "- $b_2$: treatment effect (difference between groups) **(parameter to estimate via BayesFlow)**\n",
    "- $\\text{group}$: binary indicator (0 = control, 1 = treatment)\n",
    "- $\\epsilon \\sim N(0, 1)$: residual error with **FIXED variance = 1**\n",
    "\n",
    "**Key features:**\n",
    "- No intercept term ($\\beta_0 = 0$)\n",
    "- Only 1 parameter estimated: $b_2$ (treatment effect)\n",
    "- $b_1$ (covariate effect) is simulated but not inferred\n",
    "- Error variance $\\sigma^2 = 1$ (not estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nif not os.environ.get(\"KERAS_BACKEND\"):\n    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n    \nfrom pathlib import Path\nimport importlib\n\nimport numpy as np\nnp.set_printoptions(suppress=True)\nRNG = np.random.default_rng(2025)\n\nfrom itertools import product\n\nimport keras\nimport bayesflow as bf\n\n# plotting\nimport matplotlib.pyplot as plt\n\n# Import ANCOVA-specific functions and configs from the package\nfrom rctbp_bf_training.models.ancova.model import (\n    ANCOVAConfig,\n    create_adapter,\n    create_simulator,\n    create_ancova_workflow_components,\n    get_ancova_adapter_spec,\n    simulate_cond_batch,\n    get_model_metadata,\n    save_model_with_metadata,\n    load_model_with_metadata,\n    create_validation_grid,\n    make_simulate_fn,\n    make_infer_fn,\n)\n\n# Import generic infrastructure\nfrom rctbp_bf_training.core.infrastructure import (\n    SummaryNetworkConfig,\n    InferenceNetworkConfig,\n    WorkflowConfig,\n)\nfrom rctbp_bf_training.core.utils import (\n    MovingAverageEarlyStopping,\n    loguniform_int,  # Used in plotting cells\n)\nfrom rctbp_bf_training.core.validation import (\n    run_validation_pipeline,\n    make_bayesflow_infer_fn,\n)\n\n# Create default configuration\nconfig = ANCOVAConfig()\nprint(f\"Config loaded. Model type: ancova_cont_2arms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Simulator Components\n\nThe simulator is built from three functions (defined in `ancova_cont_2arms_fns.py`):\n- **prior**: Samples `b_covariate` and `b_group` parameters\n- **likelihood**: Simulates ANCOVA data given parameters\n- **meta**: Samples context variables (N, p_alloc, prior_df, prior_scale)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prior, likelihood, meta functions are all in ancova_cont_2arms_fns\n# We use create_simulator() to build the complete simulator with config injection\nprint(\"Simulator functions defined in ancova_cont_2arms_fns:\")\nprint(\"  - prior(prior_df, prior_scale, config, rng)\")\nprint(\"  - likelihood(b_covariate, b_group, N, p_alloc, rng)\")\nprint(\"  - meta(config, rng)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create simulator using factory function (injects config and RNG)\nsimulator = create_simulator(config, RNG)\n\n# Test sampling\nsim_draws = simulator.sample(100)\nprint(\"Simulator output keys:\", sim_draws.keys())\nprint(f\"N: {sim_draws['N']}\")\nprint(f\"p_alloc: {sim_draws['p_alloc']:.3f}\")\nprint(f\"prior_df: {sim_draws['prior_df']}\")\nprint(f\"prior_scale: {sim_draws['prior_scale']:.3f}\")\nprint(f\"b_covariate shape: {sim_draws['b_covariate'].shape}\")\nprint(f\"b_group shape: {sim_draws['b_group'].shape}\")\nprint(f\"outcome[0] shape: {sim_draws['outcome'][0].shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Gamma distribution for prior_scale\n",
    "from scipy import stats\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Gamma(shape=2, scale=1) for prior_scale\n",
    "x_gamma = np.linspace(0, 8, 200)\n",
    "shape, scale = 2, 1\n",
    "gamma_pdf = stats.gamma.pdf(x_gamma, a=shape, scale=scale)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x_gamma, gamma_pdf, 'b-', lw=2)\n",
    "ax.fill_between(x_gamma, gamma_pdf, alpha=0.3)\n",
    "ax.axvline(shape * scale, color='r', ls='--', label=f'Mean = {shape * scale}')\n",
    "ax.axvline((shape - 1) * scale, color='g', ls='--', label=f'Mode = {(shape - 1) * scale}')\n",
    "ax.set_xlabel('prior_scale')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Gamma(shape={shape}, scale={scale}) for prior_scale')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 8)\n",
    "\n",
    "# Log-uniform distribution for prior_df (0 to 30)\n",
    "ax = axes[1]\n",
    "# Sample from the actual distribution\n",
    "samples_df = [int(round(loguniform_int(1, 31, alpha=0.7) - 1)) for _ in range(10000)]\n",
    "ax.hist(samples_df, bins=np.arange(-0.5, 31.5, 1), density=True, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('prior_df')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Log-uniform(1, 31, α=0.7) - 1 for prior_df')\n",
    "ax.set_xlim(-1, 31)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"prior_scale: Gamma(shape=2, scale=1) → mean={shape*scale}, mode={(shape-1)*scale}\")\n",
    "print(f\"prior_df: integer in [0, 30], log-uniform with α=0.7 (skewed toward lower values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter\n",
    "\n",
    "Transform simulator output for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Adapter created via factory function\nadapter = create_adapter()\n    \n# Test the adapter\nprocessed = adapter(sim_draws)\nprint(\"Processed data shapes:\")\nprint(\"  inference_variables:\", processed[\"inference_variables\"].shape)\nprint(\"  inference_conditions:\", processed[\"inference_conditions\"].shape)\nprint(\"  summary_variables:\", processed[\"summary_variables\"].shape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks and Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Current API Overview\n\nThe package uses a decoupled, factory-based API for maximum flexibility:\n\n**Configuration System:**\n- `ANCOVAConfig`: Top-level configuration bundling all settings\n  - `WorkflowConfig`: Generic workflow configuration (networks + training)\n    - `SummaryNetworkConfig`: Independent summary network settings\n    - `InferenceNetworkConfig`: Independent inference network settings\n    - `TrainingConfig`: Training hyperparameters\n  - `PriorConfig`: ANCOVA-specific prior settings\n  - `MetaConfig`: ANCOVA-specific meta-parameter ranges\n\n**Factory Functions:**\n- `create_simulator()`: Creates BayesFlow simulator from config\n- `create_adapter()`: Creates adapter for data transformation\n- `create_ancova_workflow_components()`: Creates all components (summary_net, inference_net, adapter) at once\n\nThis design allows:\n- Independent tuning of summary and inference networks\n- Easy swapping of network architectures\n- Reuse across different models\n- Clear separation of model-specific and generic code",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# HYPERPARAMETERS (from config, can be overridden)\n# =============================================================================\n# Access network configuration through the config object\n\n# Print current configuration\nprint(\"Network configuration:\")\nprint(f\"  Summary network: dim={config.workflow.summary_network.summary_dim}, \"\n      f\"depth={config.workflow.summary_network.depth}, \"\n      f\"width={config.workflow.summary_network.width}, \"\n      f\"dropout={config.workflow.summary_network.dropout}\")\nprint(f\"  Inference network: depth={config.workflow.inference_network.depth}, \"\n      f\"hidden={config.workflow.inference_network.hidden_sizes}, \"\n      f\"dropout={config.workflow.inference_network.dropout}\")\nprint(f\"\\nTraining configuration:\")\nprint(f\"  lr={config.workflow.training.initial_lr}, \"\n      f\"batch={config.workflow.training.batch_size}, \"\n      f\"epochs={config.workflow.training.epochs}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build networks using the ANCOVA factory function\n# This creates all workflow components at once using the current API\nsummary_network, inference_network, adapter = create_ancova_workflow_components(config)\n\nprint(f\"Networks created:\")\nprint(f\"  Summary: {summary_network}\")\nprint(f\"  Inference: {inference_network}\")\nprint(f\"  Adapter spec: {get_ancova_adapter_spec().set_keys}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimizer with decaying learning rate\ntrain_config = config.workflow.training\nsteps_per_epoch = train_config.batch_size * train_config.batches_per_epoch\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=train_config.initial_lr,\n    decay_steps=steps_per_epoch,\n    decay_rate=train_config.decay_rate,  \n    staircase=True,\n)\noptimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n\n# Create workflow using components from factory\nworkflow = bf.BasicWorkflow(\n    simulator=simulator,\n    adapter=adapter,\n    inference_network=inference_network,\n    summary_network=summary_network,\n    optimizer=optimizer,\n    inference_conditions=[\"N\", \"p_alloc\", \"prior_df\", \"prior_scale\"],\n    checkpoint_name=\"ancova_cont_2arms\"\n)\n\nprint(\"Workflow created (model will be built on first forward pass)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train the approximator to learn the posterior distribution of `b_covariate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MovingAverageEarlyStopping imported from utils (see imports cell)\nprint(\"MovingAverageEarlyStopping available from utils\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "callback = MovingAverageEarlyStopping(\n    window=train_config.early_stopping_window, \n    patience=train_config.early_stopping_patience\n)\n\nhistory = workflow.fit_online(\n    epochs=train_config.epochs, \n    batch_size=train_config.batch_size,     \n    num_batches_per_epoch=train_config.batches_per_epoch, \n    validation_data=train_config.validation_sims,\n    callbacks=[callback]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plots.loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Validate the trained approximator with simulation-based calibration and recovery diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-in-One Diagnostic Plots\n",
    "\n",
    "Generate all standard diagnostic plots at once using the built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of posterior draws you want to get\n",
    "num_samples = 1000\n",
    "\n",
    "# Simulate validation data (unseen during training)\n",
    "val_sims = simulator.sample(1000)\n",
    "\n",
    "# Obtain num_samples samples of the parameter posterior for every validation dataset\n",
    "post_draws = workflow.sample(conditions=val_sims, num_samples=num_samples)\n",
    "\n",
    "# Compute metrics for the validation set\n",
    "metrics = workflow.compute_default_diagnostics(test_data=val_sims)\n",
    "metrics[\"b_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sims.keys())\n",
    "print(val_sims['N'])\n",
    "print(val_sims['p_alloc'])\n",
    "print(val_sims['group'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import diagnostic plotting functions from the package\nfrom rctbp_bf_training.plotting import diagnostics as plot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Diagnostic Dashboard\n",
    "\n",
    "All calibration diagnostics in one comprehensive view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diagnostic dashboard using BayesFlow plots + coverage difference\n",
    "fig = plot.plot_diagnostic_dashboard(\n",
    "    estimates=post_draws,\n",
    "    targets=val_sims,\n",
    "    param_key=\"b_group\",\n",
    "    variable_name=r\"$b_2$ (treatment effect)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation on Conditions Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Function\n",
    "Function to simulate and fit one condition on a grid of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validation functions are all imported from the package (see imports cell)\n# - make_simulate_fn: creates simulation function for validation\n# - make_bayesflow_infer_fn: creates inference function wrapper\n# - run_validation_pipeline: runs complete validation on condition grid\n\nprint(\"Validation functions available from package:\")\nprint(\"  - make_simulate_fn from rctbp_bf_training.models.ancova.model\")\nprint(\"  - make_bayesflow_infer_fn from rctbp_bf_training.core.validation\")\nprint(\"  - run_validation_pipeline from rctbp_bf_training.core.validation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges for the validation grid\n",
    "b_covariate_vals = [-0.5, 0.5]\n",
    "b_group_vals = [0.0, 0.5, 1.0]\n",
    "N_vals = [20, 1000]\n",
    "p_alloc_vals = [0.5]\n",
    "prior_df_vals = [0, 3]  # 0 = Normal, 10 = moderate t, 50 = nearly Normal\n",
    "prior_scale_vals = [0.1, 1, 10]\n",
    "\n",
    "# Build conditions grid with proper format for run_validation_pipeline\n",
    "conditions = []\n",
    "for idx, (n, p, b_cov, b_grp, pdf, psc) in enumerate(\n",
    "    product(N_vals, p_alloc_vals, b_covariate_vals, b_group_vals, prior_df_vals, prior_scale_vals)\n",
    "):\n",
    "    conditions.append({\n",
    "        \"id_cond\": idx,\n",
    "        \"n_total\": n,\n",
    "        \"p_alloc\": p,\n",
    "        \"b_covariate\": b_cov,\n",
    "        \"b_arm_treat\": b_grp,  # true treatment effect\n",
    "        \"prior_df\": pdf,\n",
    "        \"prior_scale\": psc,\n",
    "    })\n",
    "\n",
    "print(f\"Total conditions: {len(conditions)}\")\n",
    "print(f\"Example condition: {conditions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the trained workflow's approximator (or load from checkpoint)\n# model_path = \"checkpoints/ancova_cont_2arms.keras\"\n# model_ancova = keras.saving.load_model(model_path)\nmodel_ancova = workflow.approximator\n\n# Create adapters using imported factory functions\nsimulate_fn = make_simulate_fn(rng=RNG)\ninfer_fn = make_bayesflow_infer_fn(\n    model_ancova, \n    param_key=\"b_group\",\n    data_keys=[\"outcome\", \"covariate\", \"group\"],\n    context_keys={\"N\": int, \"p_alloc\": float, \"prior_df\": float, \"prior_scale\": float}\n)\n\n# Run the complete validation pipeline\nresults = run_validation_pipeline(\n    conditions_list=conditions,\n    n_sims=1000,\n    n_post_draws=1000,\n    simulate_fn=simulate_fn,\n    infer_fn=infer_fn,\n    true_param_key=\"b_arm_treat\",\n    verbose=True\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "metrics = results[\"metrics\"]\n",
    "metrics[\"condition_metrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "metrics = results[\"metrics\"]\n",
    "\n",
    "# Condition-level summary statistics\n",
    "display(metrics[\"condition_summary\"].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot comprehensive SBC diagnostics (4 panels: histogram, ECDF, coverage, recovery)\nfrom rctbp_bf_training.plotting import diagnostics as plot\nimportlib.reload(plot)\n\n# Works directly on the metrics dict - shows all conditions combined\nfig = plot.plot_sbc_diagnostics(metrics)\n\n# Print key SBC test statistics\nprint(f\"SBC KS p-value: {metrics['summary']['sbc_ks_pvalue']:.4f}\")\nprint(f\"SBC C2ST accuracy: {metrics['summary']['sbc_c2st_accuracy']:.4f} (0.5 = well-calibrated)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plot)\n",
    "max_conditions=len(metrics['condition_metrics'])\n",
    "fig = plot.plot_histogram_by_condition(metrics, max_conditions=max_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model with metadata\nmodel_path = \"checkpoints/ancova_cont_2arms\"\nloaded_model, loaded_metadata = load_model_with_metadata(model_path)\n\nprint(\"Model loaded successfully\")\nif loaded_metadata:\n    print(f\"Created: {loaded_metadata.get('created_at', 'unknown')}\")\n    print(f\"Model type: {loaded_metadata.get('model_type', 'unknown')}\")"
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py311_sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}